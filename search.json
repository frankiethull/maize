[{"path":"https://frankiethull.github.io/maize/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 maize authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"https://frankiethull.github.io/maize/articles/SVMs.html","id":"corn-kernels-and-svm-kernels","dir":"Articles","previous_headings":"Example Overview","what":"corn kernels and svm kernels","title":"SVMs","text":"say three types corn different size kernels create different height corn. (1) create support vector machine can predict height corn given kernel type kernel size, also (2) predict corn type given kernel size corn height.","code":""},{"path":"https://frankiethull.github.io/maize/articles/SVMs.html","id":"corn-kernel-dataset","dir":"Articles","previous_headings":"Example Overview > corn kernels and svm kernels","what":"corn kernel dataset","title":"SVMs","text":"","code":"library(parsnip) library(maize) library(ggplot2)   # use this later to create a classification field # ----------------------------------------------- kernel_min <- corn_data$kernel_size |> min() kernel_max <- corn_data$kernel_size |> max() kernel_vec <- seq(kernel_min, kernel_max, by = 1) height_min <- corn_data$height |> min() height_max <- corn_data$height |> max()  height_vec <- seq(height_min, height_max, by = 1) corn_grid <- expand.grid(kernel_size = kernel_vec, height = height_vec) # -----------------------------------------------  maize::corn_data |>     ggplot() +     geom_point(aes(x = kernel_size, y = height, color = type)) +     theme_minimal() +     labs(title = 'corn kernel data')"},{"path":[]},{"path":"https://frankiethull.github.io/maize/articles/SVMs.html","id":"regression-for-corn-kernel-height-given-a-specialty-svm-kernel","dir":"Articles","previous_headings":"Regression","what":"regression for corn kernel height given a specialty svm kernel","title":"SVMs","text":"","code":"set.seed(31415) corn_train <- corn_data |> dplyr::sample_frac(.80) corn_test  <- corn_data |> dplyr::anti_join(corn_train) #> Joining with `by = join_by(height, kernel_size, type)`  # model params --   svm_reg_spec <-      svm_bessel(cost = 1, margin = 0.1) |>      set_mode(\"regression\") |>     set_engine(\"kernlab\")   # fit --    svm_reg_fit <- svm_reg_spec |> fit(height ~ ., data = corn_train) #>  Setting default kernel parameters  # predictions -- preds <- predict(svm_reg_fit, corn_test)  # plot -- corn_test |>   cbind(preds) |>      ggplot() +     geom_point(aes(x = kernel_size, y = height, color = type), shape = 1, size = 2) +     geom_point(aes(x = kernel_size, y = .pred,  color = type), shape = 2, size = 3) +     theme_minimal() +     labs(title = 'Bessel Kernel SVM',          subtitle = \"corn height prediction\")"},{"path":[]},{"path":"https://frankiethull.github.io/maize/articles/SVMs.html","id":"classification-of-corn-kernel-type-given-a-specialty-svm-kernel","dir":"Articles","previous_headings":"Classification","what":"classification of corn kernel type given a specialty svm kernel","title":"SVMs","text":"","code":"# model params --   svm_cls_spec <-      svm_laplace(cost = 1, margin = 0.1, laplace_sigma = 10) |>      set_mode(\"classification\") |>     set_engine(\"kernlab\")    # fit --   svm_cls_fit <- svm_cls_spec |> fit(type ~ ., data = corn_train)  # predictions -- preds <- predict(svm_cls_fit, corn_grid, \"class\") pred_grid <- corn_grid |> cbind(preds)  # plot -- corn_test |>   ggplot() +   geom_tile(inherit.aes = FALSE,             data = pred_grid,              aes(x = kernel_size, y = height, fill = .pred_class),             alpha = .8) +    geom_point(aes(x = kernel_size, y = height, color = type, shape = type), size = 3) +   theme_minimal() +   labs(subtitle = \"Laplacian Kernel\") +   scale_fill_viridis_d() +   scale_color_manual(values = c(\"violet\", \"cyan\", \"orange\")) # model params --   svm_cls_spec <-      svm_cossim(cost = 1, margin = 0.1) |>      set_mode(\"classification\") |>     set_engine(\"kernlab\")    # fit --   svm_cls_fit <- svm_cls_spec |> fit(type ~ ., data = corn_train)  # predictions -- preds <- predict(svm_cls_fit, corn_grid, \"class\") pred_grid <- corn_grid |> cbind(preds)  # plot -- corn_test |>   ggplot() +   geom_tile(inherit.aes = FALSE,             data = pred_grid,              aes(x = kernel_size, y = height, fill = .pred_class),             alpha = .8) +    geom_point(aes(x = kernel_size, y = height, color = type, shape = type), size = 3) +   theme_minimal() +   labs(subtitle = \"Cossim Similarity Kernel\") +   scale_fill_viridis_d() +   scale_color_manual(values = c(\"violet\", \"cyan\", \"orange\")) # model params --   svm_cls_spec <-      svm_anova_rbf(cost = 1, margin = 0.1) |>      set_mode(\"classification\") |>     set_engine(\"kernlab\")    # fit --   svm_cls_fit <- svm_cls_spec |> fit(type ~ ., data = corn_train) #>  Setting default kernel parameters  # predictions -- preds <- predict(svm_cls_fit, corn_grid, \"class\") pred_grid <- corn_grid |> cbind(preds)  # plot -- corn_test |>   ggplot() +   geom_tile(inherit.aes = FALSE,             data = pred_grid,              aes(x = kernel_size, y = height, fill = .pred_class),             alpha = .8) +    geom_point(aes(x = kernel_size, y = height, color = type, shape = type), size = 3) +   theme_minimal() +   labs(subtitle = \"ANOVA RBF Kernel\") +   scale_fill_viridis_d() +   scale_color_manual(values = c(\"violet\", \"cyan\", \"orange\")) # model params --   svm_cls_spec <-      svm_tanimoto(cost = 1, margin = 0.1) |>      set_mode(\"classification\") |>     set_engine(\"kernlab\")    # fit --   svm_cls_fit <- svm_cls_spec |> fit(type ~ ., data = corn_train)  # predictions -- preds <- predict(svm_cls_fit, corn_grid, \"class\") pred_grid <- corn_grid |> cbind(preds)  # plot -- corn_test |>   ggplot() +   geom_tile(inherit.aes = FALSE,             data = pred_grid,              aes(x = kernel_size, y = height, fill = .pred_class),             alpha = .8) +    geom_point(aes(x = kernel_size, y = height, color = type, shape = type), size = 3) +   theme_minimal() +   labs(subtitle = \"Tanimoto Kernel\") +   scale_fill_viridis_d() +   scale_color_manual(values = c(\"violet\", \"cyan\", \"orange\")) # model params --   svm_cls_spec <-      svm_tstudent(cost = 1, margin = 0.1, degree = 3) |>      set_mode(\"classification\") |>     set_engine(\"kernlab\")    # fit --   svm_cls_fit <- svm_cls_spec |> fit(type ~ ., data = corn_train)  # predictions -- preds <- predict(svm_cls_fit, corn_grid, \"class\") pred_grid <- corn_grid |> cbind(preds)  # plot -- corn_test |>   ggplot() +   geom_tile(inherit.aes = FALSE,             data = pred_grid,              aes(x = kernel_size, y = height, fill = .pred_class),             alpha = .8) +    geom_point(aes(x = kernel_size, y = height, color = type, shape = type), size = 3) +   theme_minimal() +   labs(subtitle = \"T-Student Kernel\") +   scale_fill_viridis_d() +   scale_color_manual(values = c(\"violet\", \"cyan\", \"orange\"))"},{"path":"https://frankiethull.github.io/maize/articles/ensembling_weak_learners.html","id":"boosting-bagging-with-ebmc-maize","dir":"Articles","previous_headings":"","what":"Boosting & Bagging with {ebmc} & {maize}","title":"ensembling weak learners in maize","text":"{ebmc} package supports binary classification tasks, response variable encoded [1, 0]. package provides implementations several boosting bagging methods tailored imbalanced datasets. handful ported {maize} including: ebmc::ub(), ebmc::rus(), ebmc::adam2(), configured use e1071::svm() base learning algorithm ensemble modeling. {maize}, current kernel supported radial basis function. Extending {baguette}’s implementation SVM’s may added later date note backend currently {ebmc} technique differs.","code":""},{"path":"https://frankiethull.github.io/maize/articles/ensembling_weak_learners.html","id":"random-under-sampling-rus-bagging-with-svms","dir":"Articles","previous_headings":"Boosting & Bagging with {ebmc} & {maize}","what":"Random Under Sampling (RUS) Bagging with SVMs","title":"ensembling weak learners in maize","text":"","code":"# model params -- bagged_svm_spec <-    bag_svm_rbf(num_learners = 50,                imb_ratio = 1) |>    set_mode(\"classification\") |>   set_engine(\"ebmc\")  # fit -- bag_svm_class_fit <- bagged_svm_spec |> fit(type ~ ., data = corn_train)  # predictions -- preds <- predict(bag_svm_class_fit, corn_grid, \"class\") pred_grid <- corn_grid |> cbind(preds)  # plot -- corn_test |>   ggplot() +   geom_tile(inherit.aes = FALSE,             data = pred_grid,              aes(x = kernel_size, y = height, fill = .pred_class),             alpha = .8) +    geom_point(aes(x = kernel_size, y = height, color = type, shape = type), size = 3) +   theme_minimal() +   labs(title = \"RUS Bagging with RBF SVMs\",        subtitle = \"corn class prediction\") +   scale_fill_viridis_d() +   scale_color_manual(values = c(\"violet\", \"orange\"))"},{"path":"https://frankiethull.github.io/maize/articles/ensembling_weak_learners.html","id":"random-under-sampling-with-boosting-rusboost-with-svms","dir":"Articles","previous_headings":"Boosting & Bagging with {ebmc} & {maize}","what":"Random Under Sampling with Boosting, RUSBoost with SVMs","title":"ensembling weak learners in maize","text":"","code":"# model params -- boost_svm_spec <-    rus_boost_svm_rbf(num_learners = 50,                  imb_ratio = 1) |>    set_mode(\"classification\") |>   set_engine(\"ebmc\")  # fit -- boost_svm_class_fit <- boost_svm_spec |> fit(type ~ ., data = corn_train)  # predictions -- preds <- predict(boost_svm_class_fit, corn_grid, \"class\") pred_grid <- corn_grid |> cbind(preds)  # plot -- corn_test |>   ggplot() +   geom_tile(inherit.aes = FALSE,             data = pred_grid,              aes(x = kernel_size, y = height, fill = .pred_class),             alpha = .8) +    geom_point(aes(x = kernel_size, y = height, color = type, shape = type), size = 3) +   theme_minimal() +   labs(title = \"RUS Boosting with RBF SVMs\",        subtitle = \"corn class prediction\") +   scale_fill_viridis_d() +   scale_color_manual(values = c(\"violet\", \"orange\"))"},{"path":"https://frankiethull.github.io/maize/articles/ensembling_weak_learners.html","id":"adaptive-boosting-adaboost-with-svms","dir":"Articles","previous_headings":"Boosting & Bagging with {ebmc} & {maize}","what":"Adaptive Boosting, AdaBoost with SVMs","title":"ensembling weak learners in maize","text":"","code":"# model params -- adaboost_svm_spec <-    ada_boost_svm_rbf(num_learners = 50,                  imb_ratio = 1) |>    set_mode(\"classification\") |>   set_engine(\"ebmc\")  # fit -- adaboost_svm_class_fit <- adaboost_svm_spec |> fit(type ~ ., data = corn_train)  # predictions -- preds <- predict(adaboost_svm_class_fit, corn_grid, \"class\") pred_grid <- corn_grid |> cbind(preds)  # plot -- corn_test |>   ggplot() +   geom_tile(inherit.aes = FALSE,             data = pred_grid,              aes(x = kernel_size, y = height, fill = .pred_class),             alpha = .8) +    geom_point(aes(x = kernel_size, y = height, color = type, shape = type), size = 3) +   theme_minimal() +   labs(title = \"AdaBoosting with RBF SVMs\",        subtitle = \"corn class prediction\") +   scale_fill_viridis_d() +   scale_color_manual(values = c(\"violet\", \"orange\"))"},{"path":"https://frankiethull.github.io/maize/articles/extending_kernlab_support.html","id":"kernels-for-the-ksvm-engine","dir":"Articles","previous_headings":"","what":"kernels for the ksvm engine","title":"new engines in maize","text":"{maize} continuing path least known engines, novelties, unique ML methods. Initially, {maize} created add additional kernel support {parsnip}, outside known linear, polynomial, radial basis function kernels. added handful odd kernels within {kernlab} library. first leap, customized kernels added registered within {kernlab} {maize}, based efforts Python Julia open sourced packages.","code":""},{"path":"https://frankiethull.github.io/maize/articles/extending_kernlab_support.html","id":"svm","dir":"Articles","previous_headings":"kernels for the ksvm engine","what":"SVM","title":"new engines in maize","text":"main engine supported within {maize} kernlab::ksvm(). function gave lot flexibility around extending kernels within package. typical regression model fit like :  , package started cooking stuff, like adding {recipes} related kernel principal component analysis Hebbian algorithms, added new step_ functions package. recent developments, post-processors added point interval calibration.","code":"# model params -- svm_reg_spec <-    svm_laplace(cost = 10, margin = .01) |>    set_mode(\"regression\") |>   set_engine(\"kernlab\")  # fit -- svm_reg_fit <- svm_reg_spec |> fit(height ~ ., data = corn_train)  # predictions -- preds <- predict(svm_reg_fit, corn_test)  # plot -- corn_test |>   cbind(preds) |>      ggplot() +     geom_point(aes(x = kernel_size, y = height, color = \"truth\", shape = type), size = 3) +     geom_point(aes(x = kernel_size, y = .pred,  color = \"pred\", shape = type), size = 3, alpha = .5) +     theme_minimal() +     labs(title = 'Laplacian Kernel SVM',          subtitle = \"corn height prediction\")"},{"path":"https://frankiethull.github.io/maize/articles/extending_kernlab_support.html","id":"beyond-the-ksvm-engine","dir":"Articles","previous_headings":"","what":"beyond the ksvm engine","title":"new engines in maize","text":"package built around supporting kernels , well, support vectors, many engines added {maize}. handful within {kernlab} library, given already wrapped “ksvm”, next step adding engines within {kernlab}. {maize} also support {ebmc} methods boosting bagging SVMs {e1701}. Another handful engines come {mildsvm} - shoutout @jrosell finding . Support tested future.","code":""},{"path":"https://frankiethull.github.io/maize/articles/extending_kernlab_support.html","id":"kqr","dir":"Articles","previous_headings":"beyond the ksvm engine","what":"KQR","title":"new engines in maize","text":"kernlab::kqr() kernel quantile regression new {maize}! shows example use engine regression. currently, kernel supported maize 0.0.1.9000 “laplacian”.","code":"# model params -- kqr_reg_spec_hi <-    kqr_laplace(cost = 10, tau = 0.95) |>    set_mode(\"regression\") |>   set_engine(\"kernlab\")  kqr_reg_spec_lo <-    kqr_laplace(cost = 10, tau = 0.05) |>    set_mode(\"regression\") |>   set_engine(\"kernlab\")   # fit -- kqr_reg_fit_hi <- kqr_reg_spec_hi |> fit(height ~ ., data = corn_train) #> Using automatic sigma estimation (sigest) for RBF or laplace kernel kqr_reg_fit_lo <- kqr_reg_spec_lo |> fit(height ~ ., data = corn_train) #> Using automatic sigma estimation (sigest) for RBF or laplace kernel  # predictions -- preds_hi <- predict(kqr_reg_fit_hi, corn_test) preds_lo <- predict(kqr_reg_fit_lo, corn_test)   # plot -- corn_test |>   cbind(hi = preds_hi$.pred) |>    cbind(lo = preds_lo$.pred) |>      ggplot() +     geom_point(aes(x = kernel_size, y = height, color = \"truth\", shape = type), size = 3) +     geom_point(aes(x = kernel_size, y = hi,  color = \"hi\", shape = type), size = 3, alpha = .5) +     geom_point(aes(x = kernel_size, y = lo,  color = \"lo\", shape = type), size = 3, alpha = .5) +     theme_minimal() +     labs(title = 'Laplacian Kernel KQR',          subtitle = \"corn height prediction\")"},{"path":"https://frankiethull.github.io/maize/articles/extending_kernlab_support.html","id":"lssvm","dir":"Articles","previous_headings":"beyond the ksvm engine","what":"LSSVM","title":"new engines in maize","text":"kernlab::lssvm() least squares support vector machine new {maize}! shows example use engine classification. currently, kernel supported maize 0.0.1.9000 “laplacian”.","code":"# model params -- lssvm_cls_spec <-    lssvm_laplace(laplace_sigma = 20) |>    set_mode(\"classification\") |>   set_engine(\"kernlab\")  # fit -- lssvm_class_fit <- lssvm_cls_spec |> fit(type ~ ., data = corn_train)  # predictions -- # predictions -- preds <- predict(lssvm_class_fit, corn_grid, \"class\") pred_grid <- corn_grid |> cbind(preds)  # plot -- corn_test |>   ggplot() +   geom_tile(inherit.aes = FALSE,             data = pred_grid,              aes(x = kernel_size, y = height, fill = .pred_class),             alpha = .8) +    geom_point(aes(x = kernel_size, y = height, color = type, shape = type), size = 3) +   theme_minimal() +   labs(title = \"Laplacian Kernel LSSVM\",        subtitle = \"corn class prediction\") +   scale_fill_viridis_d() +   scale_color_manual(values = c(\"violet\", \"cyan\", \"orange\"))"},{"path":"https://frankiethull.github.io/maize/articles/extending_kernlab_support.html","id":"rvm","dir":"Articles","previous_headings":"beyond the ksvm engine","what":"RVM","title":"new engines in maize","text":"note {kernlab} {maize} implementations regression . kernlab::rvm() relevance vector machine new {maize}! shows example use engine regression. currently, kernel supported maize 0.0.1.9000 “laplacian”. additional note, relevance vector machine may run stack overflow error. typically happens RVM processing large datasets algorithm gets stuck recursive loop. cross products called many times computationally intensive due training complexity. time, may look alternative RVM library interested extending kernels C stack issues can resolved.","code":"# simplified example given training complexity: x <- seq(-20,20,0.1) |> scale() y <- sin(x)/x + rnorm(401,sd=0.03) |> scale()  df <- data.frame(x = x, y = y) train_df <- df |> head(200)  # model params -- rvm_reg_spec <-   rvm_laplace(laplace_sigma = 100) |>   set_mode(\"regression\") |>   set_engine(\"kernlab\")  # fit -- rvm_reg_fit <- rvm_reg_spec |> fit(y ~ ., data = train_df)   # predictions -- preds <- predict(rvm_reg_fit, df |> tail(100))"},{"path":"https://frankiethull.github.io/maize/articles/glass-gem.html","id":"maize-for-novelty-detection","dir":"Articles","previous_headings":"","what":"{maize} for novelty detection","title":"glass-gem","text":"extension {applicable}:“times model’s prediction taken skepticism. example, new data point substantially different training set, predicted value may suspect. chemistry, uncommon create “applicability domain” model measures amount potential extrapolation new samples training set. applicable contains different methods measure much new data point extrapolation original data ().” Within {applicable}, lies isolation forest technique anomaly detection. SVMs also known novelty detection outlier selection. isolation forest one-SVC methods shown. Note corn testing data new type added . “Rainbow” corn added differs three corn types. showcase methods pick new type. isolation forest method based isotree base code used SVM method shown next section.  implementation based one-class-svc (novelty) detection method found kernlab.","code":"if_mod <- apd_isolation(corn_train |> dplyr::select(-type), ntrees = 10, nthreads = 1) if_mod #> Applicability domain via isolation forests #>  #> Isolation Forest model #> Consisting of 10 trees #> Numeric columns: 2  isolation_scores <- score(if_mod, corn_test |> dplyr::select(-type)) head(isolation_scores) #> # A tibble: 6 × 2 #>   score score_pctl #>   <dbl>      <dbl> #> 1 0.502       82.6 #> 2 0.572       94.7 #> 3 0.447       55.0 #> 4 0.475       72.4 #> 5 0.466       68.2 #> 6 0.504       83.3 corn_test |>   bind_cols(isolation_scores) |>    ggplot() +    geom_point(aes(x = kernel_size, y = height, color = score, shape = type)) +   theme_minimal() +   scale_color_viridis_c(option = \"B\", end = .8) +   labs(title = \"isolation forest method\") svm_mod <- apd_svm_novel_detection(corn_train |> dplyr::select(-type), kernel = \"rbfdot\", nu = .1) svm_mod #> Applicability domain via SVMs #>  #> Support Vector Machine object of class \"ksvm\"  #>  #> SV type: one-svc  (novelty detection)  #>  parameter : nu = 0.1  #>  #> Gaussian Radial Basis kernel function.  #>  Hyperparameter : sigma =  1.63116623323711  #>  #> Number of Support Vectors : 49  #>  #> Objective Function Value : 32.7176  #> Training error : 0.12963  novel_scores <- score(svm_mod, corn_test |> dplyr::select(-type)) head(novel_scores) #> # A tibble: 6 × 2 #>      score score_pctl #>      <dbl>      <dbl> #> 1  0.0849       84.1  #> 2  0.0471       68.8  #> 3 -0.00638       5.81 #> 4  0.0378       64.1  #> 5  0.00298      28.8  #> 6  0.0271       57.4 corn_test |>   bind_cols(novel_scores) |>    ggplot() +    geom_point(aes(x = kernel_size, y = height, color = score, shape = type)) +   theme_minimal() +   scale_color_viridis_c(option = \"B\", end = .8, direction = -1) +   labs(title = \"SVM novelty detection method\")"},{"path":"https://frankiethull.github.io/maize/articles/glass-gem.html","id":"isolation-forest-score-method","dir":"Articles","previous_headings":"{maize} for novelty detection","what":"isolation forest score method","title":"glass-gem","text":"isolation forest method based isotree base code used SVM method shown next section.","code":"if_mod <- apd_isolation(corn_train |> dplyr::select(-type), ntrees = 10, nthreads = 1) if_mod #> Applicability domain via isolation forests #>  #> Isolation Forest model #> Consisting of 10 trees #> Numeric columns: 2  isolation_scores <- score(if_mod, corn_test |> dplyr::select(-type)) head(isolation_scores) #> # A tibble: 6 × 2 #>   score score_pctl #>   <dbl>      <dbl> #> 1 0.502       82.6 #> 2 0.572       94.7 #> 3 0.447       55.0 #> 4 0.475       72.4 #> 5 0.466       68.2 #> 6 0.504       83.3 corn_test |>   bind_cols(isolation_scores) |>    ggplot() +    geom_point(aes(x = kernel_size, y = height, color = score, shape = type)) +   theme_minimal() +   scale_color_viridis_c(option = \"B\", end = .8) +   labs(title = \"isolation forest method\")"},{"path":"https://frankiethull.github.io/maize/articles/glass-gem.html","id":"svm-novelty-detection-score-method","dir":"Articles","previous_headings":"{maize} for novelty detection","what":"SVM novelty detection score method","title":"glass-gem","text":"implementation based one-class-svc (novelty) detection method found kernlab.","code":"svm_mod <- apd_svm_novel_detection(corn_train |> dplyr::select(-type), kernel = \"rbfdot\", nu = .1) svm_mod #> Applicability domain via SVMs #>  #> Support Vector Machine object of class \"ksvm\"  #>  #> SV type: one-svc  (novelty detection)  #>  parameter : nu = 0.1  #>  #> Gaussian Radial Basis kernel function.  #>  Hyperparameter : sigma =  1.63116623323711  #>  #> Number of Support Vectors : 49  #>  #> Objective Function Value : 32.7176  #> Training error : 0.12963  novel_scores <- score(svm_mod, corn_test |> dplyr::select(-type)) head(novel_scores) #> # A tibble: 6 × 2 #>      score score_pctl #>      <dbl>      <dbl> #> 1  0.0849       84.1  #> 2  0.0471       68.8  #> 3 -0.00638       5.81 #> 4  0.0378       64.1  #> 5  0.00298      28.8  #> 6  0.0271       57.4 corn_test |>   bind_cols(novel_scores) |>    ggplot() +    geom_point(aes(x = kernel_size, y = height, color = score, shape = type)) +   theme_minimal() +   scale_color_viridis_c(option = \"B\", end = .8, direction = -1) +   labs(title = \"SVM novelty detection method\")"},{"path":"https://frankiethull.github.io/maize/articles/glass-gem.html","id":"maize-for-kernel-canonical-correlation-analysis","dir":"Articles","previous_headings":"","what":"{maize} for kernel canonical correlation analysis","title":"glass-gem","text":"Based tidymodels’ {corrr} package: “corrr package exploring correlations R. focuses creating working data frames correlations (instead matrices) can easily explored via corrr functions leveraging tools like tidyverse.” {maize} aims similar approach kernlab’s kcca function. method non-linear extension canonical correlation analysis differs CCA. Instead finding linear combinations variables maximize correlation two sets, KCCA maps data high-dimensional feature space using kernel function applies CCA space. Kernel Canonical Correlation Analysis (KCCA) non-linear extension CCA handle comparisons two variables datasets, (x, y). analysis workflow shown : kernel canonical correlations: visualizing feature space KCCA feature space:","code":"corn_set_one <- corn_data |> dplyr::sample_frac(.50) corn_set_two <- corn_data |> dplyr::anti_join(corn_set_one) #> Joining with `by = join_by(height, kernel_size, type)`  maize_kcca <-    kcca_correlate(x = corn_set_one,                   y = corn_set_two,                   num_comp = 6)  maize_kcca |>   str() #> kcor_df [900 × 5] (S3: kcor_df/tbl_df/tbl/data.frame) #>  $ component            : num [1:900] 1 1 1 1 1 1 1 1 1 1 ... #>  $ canonical_correlation: num [1:900] 0.992 0.992 0.992 0.992 0.992 ... #>  $ sample               : int [1:900] 1 2 3 4 5 6 7 8 9 10 ... #>  $ x_feature_space      : num [1:900] -0.0057 -0.01295 0.00951 -0.00222 0.00508 ... #>  $ y_feature_space      : num [1:900] -0.003173 -0.000951 -0.016546 -0.008046 -0.016327 ... maize_kcca |>   dplyr::group_by(component) |>   dplyr::slice(1) |>   dplyr::select(component, canonical_correlation) #> # A tibble: 6 × 2 #> # Groups:   component [6] #>   component canonical_correlation #>       <dbl>                 <dbl> #> 1         1                 0.992 #> 2         2                -0.992 #> 3         3                -0.828 #> 4         4                 0.828 #> 5         5                -0.807 #> 6         6                 0.807 maize_kcca |> autoplot()"},{"path":"https://frankiethull.github.io/maize/articles/glass-gem.html","id":"kernel-canonical-correlation-analysis","dir":"Articles","previous_headings":"{maize} for kernel canonical correlation analysis","what":"kernel canonical correlation analysis","title":"glass-gem","text":"Kernel Canonical Correlation Analysis (KCCA) non-linear extension CCA handle comparisons two variables datasets, (x, y). analysis workflow shown : kernel canonical correlations: visualizing feature space KCCA feature space:","code":"corn_set_one <- corn_data |> dplyr::sample_frac(.50) corn_set_two <- corn_data |> dplyr::anti_join(corn_set_one) #> Joining with `by = join_by(height, kernel_size, type)`  maize_kcca <-    kcca_correlate(x = corn_set_one,                   y = corn_set_two,                   num_comp = 6)  maize_kcca |>   str() #> kcor_df [900 × 5] (S3: kcor_df/tbl_df/tbl/data.frame) #>  $ component            : num [1:900] 1 1 1 1 1 1 1 1 1 1 ... #>  $ canonical_correlation: num [1:900] 0.992 0.992 0.992 0.992 0.992 ... #>  $ sample               : int [1:900] 1 2 3 4 5 6 7 8 9 10 ... #>  $ x_feature_space      : num [1:900] -0.0057 -0.01295 0.00951 -0.00222 0.00508 ... #>  $ y_feature_space      : num [1:900] -0.003173 -0.000951 -0.016546 -0.008046 -0.016327 ... maize_kcca |>   dplyr::group_by(component) |>   dplyr::slice(1) |>   dplyr::select(component, canonical_correlation) #> # A tibble: 6 × 2 #> # Groups:   component [6] #>   component canonical_correlation #>       <dbl>                 <dbl> #> 1         1                 0.992 #> 2         2                -0.992 #> 3         3                -0.828 #> 4         4                 0.828 #> 5         5                -0.807 #> 6         6                 0.807 maize_kcca |> autoplot()"},{"path":"https://frankiethull.github.io/maize/articles/harvestime.html","id":"maize-for-time-series","dir":"Articles","previous_headings":"","what":"{maize} for time series","title":"harvestime","text":"{maize} continues realm novel ML techniques. release, looking extending SVMs time series. work maize? one two ways: enhancing ARIMAs via recursive framework. Based notorious “Boosted” ARIMA implementation time series machine learning. maize swaps XGBoost algorithm SVM Laplacian Kernel. Within implementation, autoARIMA ARIMA models supported, without external regressors. framework leverages two model fits! First, ARIMA model fit time series regression outcome variable. Next, SVM regression fit model residuals. code follows along {modeltime}’s getting started.  Splitting data fitting two models, regular AutoARIMA, AutoARIMA + SVM Laplace Errors. parsnip minimal output: modeltime calibration autoplots:  Recursive SVMs bit different ARIMA method mentioned . framework depend traditional statistical method ARIMA, revolves around lagged features outcome variable predictor. deviates parsnip base models leans towards modeltime. maize, goal simple lagged interface SVMs, typically supported elsewhere. autoregressive machine learning technique can read {modeltime} documentation . Note many methods lagged & rolling transformations, showcase simple lag transformer. {maize} currently recursive() bindings showcases use maize modeltime’s recursive() !","code":"m750 <- m4_monthly |> dplyr::filter(id == \"M750\")  m750 |>   plot_time_series(date, value, .interactive = FALSE) # Split Data 80/20 splits <- rsample::initial_time_split(m750, prop = 0.9)  # benchmark model: model_fit_arima_no_svm <- arima_reg() |>     set_engine(engine = \"auto_arima\") |>     fit(value ~ date, data = rsample::training(splits)) #> frequency = 12 observations per 1 year  # arima SVM errors: model_fit_arima_svm <- arima_svm_laplace(     cost          = 10,     margin        = .1,     laplace_sigma = .2      ) |>     set_engine(engine = \"auto_arima_svm_laplace\") |>     fit(value ~ date + as.numeric(date) + factor(lubridate::month(date, label = TRUE), ordered = F),         data = rsample::training(splits)) #> frequency = 12 observations per 1 year  model_fit_arima_svm #> parsnip model object #>  #> ARIMA(0,1,1)(0,1,1)[12] w/ SVM Errors #> --- #> Model 1: Auto ARIMA #> Series: outcome  #> ARIMA(0,1,1)(0,1,1)[12]  #>  #> Coefficients: #>           ma1     sma1 #>       -0.3405  -0.4781 #> s.e.   0.0652   0.0628 #>  #> sigma^2 = 25114:  log likelihood = -1699.55 #> AIC=3405.1   AICc=3405.19   BIC=3415.8 #>  #> --- #> Model 2: SVM Errors #>  #> parsnip model object #>  #> Support Vector Machine object of class \"ksvm\"  #>  #> SV type: eps-svr  (regression)  #>  parameter : epsilon = 0.1  cost C = 10  #>  #> Laplace kernel function.  #>  Hyperparameter : sigma =  0.2  #>  #> Number of Support Vectors : 244  #>  #> Objective Function Value : -1115  #> Training error : 0.574759 predict(model_fit_arima_svm, rsample::testing(splits)) |> head() #> # A tibble: 6 × 1 #>    .pred #>    <dbl> #> 1 10395. #> 2 10613. #> 3 10608. #> 4 10779. #> 5 10744. #> 6 10761. models_tbl <- modeltime_table(     model_fit_arima_no_svm,     model_fit_arima_svm )  calibration_tbl <- models_tbl |>     modeltime_calibrate(new_data = rsample::testing(splits))  calibration_tbl$.model_desc <- c(\"AutoARIMA\", \"AutoARIMA + SVM Errors\") calibration_tbl |>     modeltime_forecast(         new_data    = rsample::testing(splits),         actual_data = m750     ) |>     plot_modeltime_forecast(       .legend_max_width = 25, # For mobile screens       .interactive      = FALSE     ) #> Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning #> -Inf # splits  splits   <- rsample::initial_time_split(m750, prop = 0.9) training <- rsample::training(splits)  testing  <- rsample::testing(splits)    # horizon and number of lags: horizon <- nrow(testing)  lag_transformer <- function(data){   data |>     timetk::tk_augment_lags(value, .lags = 1:horizon) } extended <- m750 |>   dplyr::group_by(id) |>   timetk::future_frame(     .length_out = horizon,     .bind_data  = TRUE   ) #> .date_var is missing. Using: date  m4_lags <- extended |>   lag_transformer()  train_data <- m4_lags |>   tidyr::drop_na()  future_data <- m4_lags |>   dplyr::filter(is.na(value))  model_fit_recursive_svm_laplace <- svm_laplace(   mode = \"regression\",   cost = 10,   margin = .1,   laplace_sigma = .2 ) |>   set_engine(\"kernlab\") |>   fit(     value ~ .     + lubridate::month(date, label = TRUE)     + as.numeric(date)     - date,     data = train_data   ) |>   modeltime::recursive(     transform  = lag_transformer,     train_tail = tail(train_data, horizon)   )    model_tbl <- modeltime_table(   model_fit_recursive_svm_laplace )   model_tbl |>    # Forecast using future data   modeltime_forecast(     new_data    = future_data,     actual_data = m750   ) |>    # Visualize the forecast   plot_modeltime_forecast(     .interactive        = FALSE,     .conf_interval_show = FALSE   ) #> Adding missing grouping variables: `id`"},{"path":"https://frankiethull.github.io/maize/articles/harvestime.html","id":"arima-svm-errors","dir":"Articles","previous_headings":"","what":"ARIMA + SVM Errors","title":"harvestime","text":"Based notorious “Boosted” ARIMA implementation time series machine learning. maize swaps XGBoost algorithm SVM Laplacian Kernel. Within implementation, autoARIMA ARIMA models supported, without external regressors. framework leverages two model fits! First, ARIMA model fit time series regression outcome variable. Next, SVM regression fit model residuals. code follows along {modeltime}’s getting started.  Splitting data fitting two models, regular AutoARIMA, AutoARIMA + SVM Laplace Errors. parsnip minimal output: modeltime calibration autoplots:","code":"m750 <- m4_monthly |> dplyr::filter(id == \"M750\")  m750 |>   plot_time_series(date, value, .interactive = FALSE) # Split Data 80/20 splits <- rsample::initial_time_split(m750, prop = 0.9)  # benchmark model: model_fit_arima_no_svm <- arima_reg() |>     set_engine(engine = \"auto_arima\") |>     fit(value ~ date, data = rsample::training(splits)) #> frequency = 12 observations per 1 year  # arima SVM errors: model_fit_arima_svm <- arima_svm_laplace(     cost          = 10,     margin        = .1,     laplace_sigma = .2      ) |>     set_engine(engine = \"auto_arima_svm_laplace\") |>     fit(value ~ date + as.numeric(date) + factor(lubridate::month(date, label = TRUE), ordered = F),         data = rsample::training(splits)) #> frequency = 12 observations per 1 year  model_fit_arima_svm #> parsnip model object #>  #> ARIMA(0,1,1)(0,1,1)[12] w/ SVM Errors #> --- #> Model 1: Auto ARIMA #> Series: outcome  #> ARIMA(0,1,1)(0,1,1)[12]  #>  #> Coefficients: #>           ma1     sma1 #>       -0.3405  -0.4781 #> s.e.   0.0652   0.0628 #>  #> sigma^2 = 25114:  log likelihood = -1699.55 #> AIC=3405.1   AICc=3405.19   BIC=3415.8 #>  #> --- #> Model 2: SVM Errors #>  #> parsnip model object #>  #> Support Vector Machine object of class \"ksvm\"  #>  #> SV type: eps-svr  (regression)  #>  parameter : epsilon = 0.1  cost C = 10  #>  #> Laplace kernel function.  #>  Hyperparameter : sigma =  0.2  #>  #> Number of Support Vectors : 244  #>  #> Objective Function Value : -1115  #> Training error : 0.574759 predict(model_fit_arima_svm, rsample::testing(splits)) |> head() #> # A tibble: 6 × 1 #>    .pred #>    <dbl> #> 1 10395. #> 2 10613. #> 3 10608. #> 4 10779. #> 5 10744. #> 6 10761. models_tbl <- modeltime_table(     model_fit_arima_no_svm,     model_fit_arima_svm )  calibration_tbl <- models_tbl |>     modeltime_calibrate(new_data = rsample::testing(splits))  calibration_tbl$.model_desc <- c(\"AutoARIMA\", \"AutoARIMA + SVM Errors\") calibration_tbl |>     modeltime_forecast(         new_data    = rsample::testing(splits),         actual_data = m750     ) |>     plot_modeltime_forecast(       .legend_max_width = 25, # For mobile screens       .interactive      = FALSE     ) #> Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning #> -Inf"},{"path":"https://frankiethull.github.io/maize/articles/harvestime.html","id":"calibrated-autoarima-vs--autoarima-w-svm-laplacian-kernel-errors-predictions","dir":"Articles","previous_headings":"","what":"Calibrated AutoARIMA vs. AutoARIMA w/ ‘SVM Laplacian Kernel’ Errors predictions:","title":"harvestime","text":"","code":"calibration_tbl |>     modeltime_forecast(         new_data    = rsample::testing(splits),         actual_data = m750     ) |>     plot_modeltime_forecast(       .legend_max_width = 25, # For mobile screens       .interactive      = FALSE     ) #> Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning #> -Inf"},{"path":"https://frankiethull.github.io/maize/articles/harvestime.html","id":"recursive-svms","dir":"Articles","previous_headings":"","what":"Recursive SVMs","title":"harvestime","text":"Recursive SVMs bit different ARIMA method mentioned . framework depend traditional statistical method ARIMA, revolves around lagged features outcome variable predictor. deviates parsnip base models leans towards modeltime. maize, goal simple lagged interface SVMs, typically supported elsewhere. autoregressive machine learning technique can read {modeltime} documentation . Note many methods lagged & rolling transformations, showcase simple lag transformer. {maize} currently recursive() bindings showcases use maize modeltime’s recursive() !","code":"# splits  splits   <- rsample::initial_time_split(m750, prop = 0.9) training <- rsample::training(splits)  testing  <- rsample::testing(splits)    # horizon and number of lags: horizon <- nrow(testing)  lag_transformer <- function(data){   data |>     timetk::tk_augment_lags(value, .lags = 1:horizon) } extended <- m750 |>   dplyr::group_by(id) |>   timetk::future_frame(     .length_out = horizon,     .bind_data  = TRUE   ) #> .date_var is missing. Using: date  m4_lags <- extended |>   lag_transformer()  train_data <- m4_lags |>   tidyr::drop_na()  future_data <- m4_lags |>   dplyr::filter(is.na(value))  model_fit_recursive_svm_laplace <- svm_laplace(   mode = \"regression\",   cost = 10,   margin = .1,   laplace_sigma = .2 ) |>   set_engine(\"kernlab\") |>   fit(     value ~ .     + lubridate::month(date, label = TRUE)     + as.numeric(date)     - date,     data = train_data   ) |>   modeltime::recursive(     transform  = lag_transformer,     train_tail = tail(train_data, horizon)   )    model_tbl <- modeltime_table(   model_fit_recursive_svm_laplace )   model_tbl |>    # Forecast using future data   modeltime_forecast(     new_data    = future_data,     actual_data = m750   ) |>    # Visualize the forecast   plot_modeltime_forecast(     .interactive        = FALSE,     .conf_interval_show = FALSE   ) #> Adding missing grouping variables: `id`"},{"path":"https://frankiethull.github.io/maize/articles/perchance.html","id":"post-processing-in-maize","dir":"Articles","previous_headings":"","what":"Post-Processing in {maize}???","title":"perchance","text":"based entirely {probably} package implementation conformal inference & calibration. Hence pun perchance? {probably} package implements int_conformal_quantile method generating prediction intervals. method utilizes random forest quantile regression underlying algorithm. {maize} package, new method called int_conformal_quantile_svm introduced. implementation replaces original quantile regression random forest {quantregForest} approach quantile regression SVM {qrsvm}. ’s worth noting int_conformal_quantile_svm method relies {qrsvm} package, currently available CRAN.","code":""},{"path":"https://frankiethull.github.io/maize/articles/perchance.html","id":"key-components","dir":"Articles","previous_headings":"Post-Processing in {maize}???","what":"Key Components","title":"perchance","text":"{probably} Package Method: int_conformal_quantile Algorithm: quantregForest::quantregForest() {maize} Package Method: int_conformal_quantile_svm Algorithm: qrsvm::qrsvm()","code":""},{"path":"https://frankiethull.github.io/maize/articles/perchance.html","id":"implementation-details","dir":"Articles","previous_headings":"Post-Processing in {maize}???","what":"Implementation Details","title":"perchance","text":"int_conformal_quantile_svm method {maize} represents straightforward substitution RF algorithm. Instead using random forests, employs SVMs generating prediction intervals. modification allows users leverage potential benefits SVMs certain prediction scenarios.","code":""},{"path":"https://frankiethull.github.io/maize/articles/perchance.html","id":"dependency-note","dir":"Articles","previous_headings":"Post-Processing in {maize}???","what":"Dependency Note","title":"perchance","text":"{qrsvm} package, required SVM-based implementation {maize}, currently hosted CRAN. Users interested utilizing int_conformal_quantile_svm method aware external dependency may need install github.com/frankiethull/qrsvm.","code":"library(qrsvm) library(dplyr) library(parsnip) library(maize) library(recipes) library(workflows) library(kernlab) library(probably)"},{"path":"https://frankiethull.github.io/maize/articles/perchance.html","id":"point-and-interval-regressions","dir":"Articles","previous_headings":"","what":"point and interval regressions","title":"perchance","text":"following demonstration begins initial point forecast using Support Vector Machine (SVM) Laplacian kernel. Subsequently, utilize --bag (OOB) sample construct conformal prediction intervals. comparison, evaluate int_conformal_quantile method {probably} package int_conformal_quantile_svm method {maize} package. analysis provide insights performance characteristics two approaches generating prediction intervals.","code":"set.seed(31415)    # note: qrsvm does not work for non-numeric values corn_df <- corn_data |> mutate(type = as.numeric(factor(type, levels = levels(type))))  corn_train <- corn_df |> dplyr::sample_frac(.80) corn_cal   <- corn_df |> dplyr::anti_join(corn_train) |> head(30) corn_test  <- corn_df |> dplyr::anti_join(corn_train) |> tail(30)   # SVM base model  svm_spec <-   svm_laplace() |>   set_mode(\"regression\")  svm_wflow <-   workflow() |>   add_model(svm_spec) |>   add_formula(height ~ .)  svm_fit <- fit(svm_wflow, data = corn_train)   # probably's implementation: rf_int <- probably::int_conformal_quantile(svm_fit, corn_train, corn_cal,   level = 0.80 )  # experimental QRSVM interface for CQR:  svm_int <- int_conformal_quantile_svm(svm_fit, corn_train, corn_cal,   level = 0.80, cost = 1000, degree = 5 )  svm_int #> Split Conformal inference via Quantile Regression SVM #> preprocessor: formula  #> model: svm_laplace (engine = kernlab)  #> calibration set size: 30  #> confidence level: 0.8  #>  #> Use `predict(object, new_data)` to compute prediction intervals"},{"path":"https://frankiethull.github.io/maize/articles/perchance.html","id":"predict-with-intervals","dir":"Articles","previous_headings":"","what":"predict with intervals","title":"perchance","text":"","code":"cqr_rf  <- predict(rf_int,  corn_test) cqr_svm <- predict(svm_int, corn_test)   # renaming pre-plotting names(cqr_rf)  <- c(\".pred\", \".pred_lo_rf\",  \".pred_hi_rf\") names(cqr_svm) <- c(\".pred\", \".pred_lo_svm\", \".pred_hi_svm\")"},{"path":[]},{"path":"https://frankiethull.github.io/maize/articles/perchance.html","id":"svm-with-qrf-qrsvm-prediction-intervals","dir":"Articles","previous_headings":"","what":"SVM with QRF & QRSVM Prediction Intervals","title":"perchance","text":"","code":"library(ggplot2) #>  #> Attaching package: 'ggplot2' #> The following object is masked from 'package:kernlab': #>  #>     alpha  corn_test |>   dplyr::bind_cols(cqr_svm) |>   dplyr::bind_cols(cqr_rf |> dplyr::select(-.pred)) |>   ggplot() +   geom_ribbon(aes(x = kernel_size, ymin = .pred_lo_svm, ymax = .pred_hi_svm, fill = \"QR-SVM\"), alpha = .5) +   geom_ribbon(aes(x = kernel_size, ymin = .pred_lo_rf,  ymax = .pred_hi_rf,  fill = \"QR-RF\"),  alpha = .5) +   geom_point(aes(x = kernel_size, y = height, color = \"truth\")) +   geom_point(aes(x = kernel_size, y = .pred,  color = \"pred\")) +    theme_minimal() +   labs(subtitle = \"Corn Height Prediction Intervals\")"},{"path":"https://frankiethull.github.io/maize/articles/perchance.html","id":"svm-with-linear-svm-calibration","dir":"Articles","previous_headings":"","what":"SVM with Linear & SVM Calibration","title":"perchance","text":"addition conformal prediction intervals, {maize} package offers post-process calibration engine similar one {probably}, utilizes Support Vector Machines (SVMs). {probably} package includes cal_estimate_linear function, calibrates point forecasts using post-model address bias initial fit. {maize} provides comparable function called cal_estimate_svm. function follows similar framework cal_estimate_linear, employs either vanilla polynomial kernel, depending user’s selection “smooth” argument. following section demonstrates application calibration approach.","code":""},{"path":"https://frankiethull.github.io/maize/articles/perchance.html","id":"key-components-1","dir":"Articles","previous_headings":"SVM with Linear & SVM Calibration","what":"Key Components","title":"perchance","text":"{probably} Package Method: cal_estimate_linear Algorithms: - stats::glm() used smooth set FALSE - mgcv::gam() used smooth set TRUE {maize} Package Method: cal_estimate_svm Algorithms: - kernlab::ksvm(kernel = \"vanilladot\") used smooth set FALSE - kernlab::ksvm(kernel = \"polydot\") used smooth set TRUE","code":""},{"path":"https://frankiethull.github.io/maize/articles/perchance.html","id":"implementation-details-1","dir":"Articles","previous_headings":"SVM with Linear & SVM Calibration","what":"Implementation Details","title":"perchance","text":"cal_estimate_svm method {maize} represents straightforward substitution linear calibration. Instead using linear models, employs SVMs post-calibration. modification allows users leverage potential benefits SVMs certain prediction scenarios.","code":"calibration_df <-  predict(svm_fit, corn_cal) |> dplyr::bind_cols(corn_cal)   # probably's implementation: lin_cal_fit <- calibration_df |>              probably::cal_estimate_linear(truth = height, estimate = .pred, smooth = TRUE) #> Registered S3 method overwritten by 'butcher': #>   method                 from     #>   as.character.dev_topic generics  # experimental svm calibration (vanilladot or polydot) svm_cal_fit <- calibration_df |>             maize::cal_estimate_svm(height, smooth = TRUE) #>  Setting default kernel parameters  # preds cal_corn_test <-  predict(svm_fit, corn_test) |>   cbind(corn_test)    # calibrated preds: maize::cal_apply_regression(svm_cal_fit, cal_corn_test) |> head(10) #>       .pred   height kernel_size type #> 1  64.42768 62.61279   11.051488    2 #> 2  65.53022 71.24822   10.629400    2 #> 3  66.25433 78.86852   11.401994    2 #> 4  68.00431 69.60042    9.926075    2 #> 5  66.04308 70.01313   11.738339    2 #> 6  70.82399 66.84150    9.685755    2 #> 7  70.70666 69.62163    9.648538    2 #> 8  67.20650 75.24819   10.028623    2 #> 9  66.54068 69.38940    8.890259    2 #> 10 68.10534 60.64923    9.923742    2"},{"path":"https://frankiethull.github.io/maize/articles/pre-processors-in-maize.html","id":"kernel-recipes","dir":"Articles","previous_headings":"","what":"kernel recipes","title":"pre-processors in maize","text":"{recipes} package within tidymodels ecosystem. main goal recipes create steps pre-processors datasets. steps can useful simplifying large datasets key components, dropping NA values, creating lags, . steps create new features can used regression classification model. Recipes {maize} focus specialty kernel methods extracting components/features, thanks {kernlab} {mildsvm} packages. apply recipe steps maize directly corn data. glimpse original dataset. corn dataset three types corn vary height kernel size.  Kernel Principal Components Analysis nonlinear form principal component analysis. Laplacian tanh kernels supported step_kpca_*.  Kernel Hebbian Algorithm nonlinear iterative algorithm principal component analysis. Laplacian tanh kernels supported step_kha_*.  Kernel Feature Analysis algorithm algorithm extracting structure possibly high-dimensional data sets. Similar kpca new basis data found. data can projected new basis. laplacian kernel supported step_kfa_laplace.  Use Nyström method, step_kfm_nystrom fit feature map approximates ‘radial’ kernel.","code":"corn_data |>   ggplot(aes(x = kernel_size, y = height, color = type, shape = type)) +   geom_point(size = 3) +   theme_minimal() +   labs(title = 'Original Unprocessed Data',        subtitle = \"corn dataset\") +   scale_color_viridis_d(end = .9) +   theme(legend.position = \"top\") kpca_data <-  recipe(type ~ ., corn_data) |>   step_kpca_laplace(height, num_comp = 6) |>   prep() |>   bake(new_data = corn_data)  kpca_data |>   tidyr::pivot_longer(-c(kernel_size, type)) |>   ggplot(aes(x = kernel_size, y = value, color = type, shape = type)) +   geom_point(size = 3) +   facet_wrap(~name, scales = 'free') +   theme_minimal() +   labs(title = 'Laplace kPCA Processed Data',        subtitle = \"corn dataset\") +   scale_color_viridis_d(end = .9) +   theme(legend.position = \"top\") kha_data <-  recipe(type ~ ., corn_data) |>   step_kha_laplace(height, num_comp = 3) |>   prep() |>   bake(new_data = corn_data)  kha_data |>   tidyr::pivot_longer(-c(kernel_size, type)) |>   ggplot(aes(x = kernel_size, y = value, color = type, shape = type)) +   geom_point(size = 3, alpha = .5) +   facet_wrap(~name, scales = 'free') +   theme_minimal() +   labs(title = 'Laplace kha Processed Data',        subtitle = \"corn dataset\") +   scale_color_viridis_d(end = .9) +   theme(legend.position = \"top\") kfa_data <- recipe(type ~ ., corn_data) |>   step_kfa_laplace(height, kernel_size, num_comp = 9, sigma = .87) |>   prep() |>   bake(new_data = corn_data)  kfa_data |>   tidyr::pivot_longer(-c(kFA1, type)) |>   ggplot(aes(x = kFA1, y = value, color = type, shape = type)) +   geom_point(size = 3, alpha = .5) +   facet_wrap(~name, scales = 'free') +   theme_minimal() +   labs(title = 'Laplace kfa Processed Data',        subtitle = \"corn dataset\") +   scale_color_viridis_d(end = .9) +   theme(legend.position = \"top\") kfm_data <-  recipe(type ~ ., corn_data) |>   step_kfm_nystrom(height, kernel_size, r = 10, sigma = .01) |>   prep() |>   bake(new_data = corn_data)  kfm_data |>   tidyr::pivot_longer(-c(kFM01, type)) |>   ggplot(aes(x = kFM01, y = value, color = type, shape = type)) +   geom_point(size = 3, alpha = .5) +   facet_wrap(~name, scales = 'free') +   theme_minimal() +   labs(title = 'Nystrom KFM RBF Processed Data',        subtitle = \"corn dataset\") +   scale_color_viridis_d(end = .9) +   theme(legend.position = \"top\")"},{"path":"https://frankiethull.github.io/maize/articles/pre-processors-in-maize.html","id":"corn-dataset","dir":"Articles","previous_headings":"","what":"Corn Dataset","title":"pre-processors in maize","text":"glimpse original dataset. corn dataset three types corn vary height kernel size.","code":"corn_data |>   ggplot(aes(x = kernel_size, y = height, color = type, shape = type)) +   geom_point(size = 3) +   theme_minimal() +   labs(title = 'Original Unprocessed Data',        subtitle = \"corn dataset\") +   scale_color_viridis_d(end = .9) +   theme(legend.position = \"top\")"},{"path":"https://frankiethull.github.io/maize/articles/pre-processors-in-maize.html","id":"kernel-pca","dir":"Articles","previous_headings":"","what":"Kernel PCA","title":"pre-processors in maize","text":"Kernel Principal Components Analysis nonlinear form principal component analysis. Laplacian tanh kernels supported step_kpca_*.","code":"kpca_data <-  recipe(type ~ ., corn_data) |>   step_kpca_laplace(height, num_comp = 6) |>   prep() |>   bake(new_data = corn_data)  kpca_data |>   tidyr::pivot_longer(-c(kernel_size, type)) |>   ggplot(aes(x = kernel_size, y = value, color = type, shape = type)) +   geom_point(size = 3) +   facet_wrap(~name, scales = 'free') +   theme_minimal() +   labs(title = 'Laplace kPCA Processed Data',        subtitle = \"corn dataset\") +   scale_color_viridis_d(end = .9) +   theme(legend.position = \"top\")"},{"path":"https://frankiethull.github.io/maize/articles/pre-processors-in-maize.html","id":"laplace-pca","dir":"Articles","previous_headings":"","what":"Laplace PCA","title":"pre-processors in maize","text":"","code":"kpca_data <-  recipe(type ~ ., corn_data) |>   step_kpca_laplace(height, num_comp = 6) |>   prep() |>   bake(new_data = corn_data)  kpca_data |>   tidyr::pivot_longer(-c(kernel_size, type)) |>   ggplot(aes(x = kernel_size, y = value, color = type, shape = type)) +   geom_point(size = 3) +   facet_wrap(~name, scales = 'free') +   theme_minimal() +   labs(title = 'Laplace kPCA Processed Data',        subtitle = \"corn dataset\") +   scale_color_viridis_d(end = .9) +   theme(legend.position = \"top\")"},{"path":"https://frankiethull.github.io/maize/articles/pre-processors-in-maize.html","id":"kernel-hebbian-algorithm","dir":"Articles","previous_headings":"","what":"Kernel Hebbian Algorithm","title":"pre-processors in maize","text":"Kernel Hebbian Algorithm nonlinear iterative algorithm principal component analysis. Laplacian tanh kernels supported step_kha_*.","code":"kha_data <-  recipe(type ~ ., corn_data) |>   step_kha_laplace(height, num_comp = 3) |>   prep() |>   bake(new_data = corn_data)  kha_data |>   tidyr::pivot_longer(-c(kernel_size, type)) |>   ggplot(aes(x = kernel_size, y = value, color = type, shape = type)) +   geom_point(size = 3, alpha = .5) +   facet_wrap(~name, scales = 'free') +   theme_minimal() +   labs(title = 'Laplace kha Processed Data',        subtitle = \"corn dataset\") +   scale_color_viridis_d(end = .9) +   theme(legend.position = \"top\")"},{"path":"https://frankiethull.github.io/maize/articles/pre-processors-in-maize.html","id":"laplace-hebbian","dir":"Articles","previous_headings":"","what":"Laplace Hebbian","title":"pre-processors in maize","text":"","code":"kha_data <-  recipe(type ~ ., corn_data) |>   step_kha_laplace(height, num_comp = 3) |>   prep() |>   bake(new_data = corn_data)  kha_data |>   tidyr::pivot_longer(-c(kernel_size, type)) |>   ggplot(aes(x = kernel_size, y = value, color = type, shape = type)) +   geom_point(size = 3, alpha = .5) +   facet_wrap(~name, scales = 'free') +   theme_minimal() +   labs(title = 'Laplace kha Processed Data',        subtitle = \"corn dataset\") +   scale_color_viridis_d(end = .9) +   theme(legend.position = \"top\")"},{"path":"https://frankiethull.github.io/maize/articles/pre-processors-in-maize.html","id":"kernel-feature-analysis","dir":"Articles","previous_headings":"","what":"Kernel Feature Analysis","title":"pre-processors in maize","text":"Kernel Feature Analysis algorithm algorithm extracting structure possibly high-dimensional data sets. Similar kpca new basis data found. data can projected new basis. laplacian kernel supported step_kfa_laplace.","code":"kfa_data <- recipe(type ~ ., corn_data) |>   step_kfa_laplace(height, kernel_size, num_comp = 9, sigma = .87) |>   prep() |>   bake(new_data = corn_data)  kfa_data |>   tidyr::pivot_longer(-c(kFA1, type)) |>   ggplot(aes(x = kFA1, y = value, color = type, shape = type)) +   geom_point(size = 3, alpha = .5) +   facet_wrap(~name, scales = 'free') +   theme_minimal() +   labs(title = 'Laplace kfa Processed Data',        subtitle = \"corn dataset\") +   scale_color_viridis_d(end = .9) +   theme(legend.position = \"top\")"},{"path":"https://frankiethull.github.io/maize/articles/pre-processors-in-maize.html","id":"laplace-feature-analysis","dir":"Articles","previous_headings":"","what":"Laplace Feature Analysis","title":"pre-processors in maize","text":"","code":"kfa_data <- recipe(type ~ ., corn_data) |>   step_kfa_laplace(height, kernel_size, num_comp = 9, sigma = .87) |>   prep() |>   bake(new_data = corn_data)  kfa_data |>   tidyr::pivot_longer(-c(kFA1, type)) |>   ggplot(aes(x = kFA1, y = value, color = type, shape = type)) +   geom_point(size = 3, alpha = .5) +   facet_wrap(~name, scales = 'free') +   theme_minimal() +   labs(title = 'Laplace kfa Processed Data',        subtitle = \"corn dataset\") +   scale_color_viridis_d(end = .9) +   theme(legend.position = \"top\")"},{"path":"https://frankiethull.github.io/maize/articles/pre-processors-in-maize.html","id":"nystrom-kernel-feature-map","dir":"Articles","previous_headings":"","what":"Nystrom Kernel Feature Map","title":"pre-processors in maize","text":"Use Nyström method, step_kfm_nystrom fit feature map approximates ‘radial’ kernel.","code":"kfm_data <-  recipe(type ~ ., corn_data) |>   step_kfm_nystrom(height, kernel_size, r = 10, sigma = .01) |>   prep() |>   bake(new_data = corn_data)  kfm_data |>   tidyr::pivot_longer(-c(kFM01, type)) |>   ggplot(aes(x = kFM01, y = value, color = type, shape = type)) +   geom_point(size = 3, alpha = .5) +   facet_wrap(~name, scales = 'free') +   theme_minimal() +   labs(title = 'Nystrom KFM RBF Processed Data',        subtitle = \"corn dataset\") +   scale_color_viridis_d(end = .9) +   theme(legend.position = \"top\")"},{"path":"https://frankiethull.github.io/maize/articles/two-class-post-processing.html","id":"two-types-of-corn-kernels-we-need-to-classify","dir":"Articles","previous_headings":"","what":"two types of corn kernels we need to classify:","title":"two-class-post-processing","text":"’re filter three two types corn.","code":"# binary classification problem data: two_class_df <- maize::corn_data |>                  dplyr::filter(type %in% c('Popcorn', 'Field')) |>                 droplevels()   # --------------cornfield------------------------- kernel_min <- two_class_df$kernel_size |> min() kernel_max <- two_class_df$kernel_size |> max() kernel_vec <- seq(kernel_min, kernel_max, by = 1) height_min <- two_class_df$height |> min() height_max <- two_class_df$height |> max()  height_vec <- seq(height_min, height_max, by = 1) cornfield <- expand.grid(kernel_size = kernel_vec, height = height_vec) # -----------------------------------------------"},{"path":"https://frankiethull.github.io/maize/articles/two-class-post-processing.html","id":"create-a-three-way-split-using-rsample","dir":"Articles","previous_headings":"","what":"create a three-way split using rsample:","title":"two-class-post-processing","text":"three-way splits useful post-processing calibrations. training set, validation set calibration, final test set checking post-processing steps.","code":"set.seed(42) data_split <- rsample::initial_validation_split(two_class_df)  train <- rsample::training(data_split) test  <- rsample::testing(data_split) valid <- rsample::validation(data_split)  train |>   ggplot() +   geom_point(aes(x = kernel_size, y = height, color = type, shape = type)) +   theme_minimal() +   labs(subtitle = \"training data\")"},{"path":"https://frankiethull.github.io/maize/articles/two-class-post-processing.html","id":"train-a-classification-svm-using-workflows","dir":"Articles","previous_headings":"","what":"train a classification svm using workflows:","title":"two-class-post-processing","text":"fit model, using workflow gives us flexibility downstream.","code":"# model params -- svm_cls_spec <-    svm_sorensen(cost = 1, margin = 0.1) |>    set_mode(\"classification\") |>   set_engine(\"kernlab\")    # fit -- # simply fit a model #  svm_cls_fit <- svm_cls_spec |> fit(type ~ ., data = train)  # fit with a workflow -- # workflows allow for bundled pre- and post- processing svm_wkflow <-    workflows::workflow() |>   workflows::add_model(svm_cls_spec) |>   workflows::add_formula(type ~ .)  svm_cls_fit <- fit(svm_wkflow, train)"},{"path":"https://frankiethull.github.io/maize/articles/two-class-post-processing.html","id":"probably","dir":"Articles","previous_headings":"","what":"{probably}","title":"two-class-post-processing","text":"detection, thresholding, calibration steps. let’s see confusion matrix looks like given trained model. ’s probably (pun intended) edge overlapping observations. try tune parameters perfect confusion matrix. try calibrating trained model. cases handle edge, ’s uncertain predict accurately, .e. equivocal zone. classification field vs test observations:  detection equivocal zones via probably: check EQs:  note calibration functions regression classification {probably}. ’re honing calibration classification demonstrate cal_estimate_logistic. Since calibration, run calibration step validation data, validation step used calibrate predictions test.","code":"initial_class_preds_test <- predict(svm_cls_fit, test)  test |>   dplyr::bind_cols(initial_class_preds_test) |>   yardstick::conf_mat(type, .pred_class) #>           Truth #> Prediction Field Popcorn #>    Field      15       1 #>    Popcorn     3      21 preds <- predict(svm_cls_fit, cornfield, \"class\") predfield <- cornfield |> cbind(preds)  test |>   dplyr::bind_cols(     initial_class_preds_test   ) |>   dplyr::mutate(     hit_or_miss = type == .pred_class   ) |>   ggplot() +   # plotting the classification field:     geom_tile(inherit.aes = FALSE,             data = predfield,              aes(x = kernel_size, y = height, fill = .pred_class),             alpha = .4) +    geom_point(aes(x = kernel_size, y = height, color = hit_or_miss, shape = type)) +   theme_minimal() +   labs(subtitle = \"fitted model detection vs test data\") # predict the probability output instead of class: initial_class_preds_prob_test <- predict(svm_cls_fit, test, type = \"prob\")  # probabilities and outcome: initial_class_preds_prob_test <- initial_class_preds_prob_test |>                                   dplyr::bind_cols(test |> dplyr::select(type))  # probably EQ step: class_preds_prob_eq <- initial_class_preds_prob_test |>   dplyr::mutate(     .pred_class = probably::make_two_class_pred(       estimate = .pred_Field,       levels = levels(type),       # adjust threshold and buffer to handle eq        threshold = 0.5,       buffer = 0.35     )   )  class_preds_prob_eq |>    dplyr::count(.pred_class) #> # A tibble: 3 × 2 #>   .pred_class     n #>    <clss_prd> <int> #> 1        [EQ]     4 #> 2       Field    14 #> 3     Popcorn    22 test |>   dplyr::bind_cols(     class_preds_prob_eq |> dplyr::select(.pred_class)   ) |>   dplyr::mutate(     .pred_class = as.factor(.pred_class),     hit_or_miss = type == .pred_class   ) |>   ggplot() +   # plotting the classification field:     geom_tile(inherit.aes = FALSE,             data = predfield,              aes(x = kernel_size, y = height, fill = .pred_class),             alpha = .4) +    geom_point(aes(x = kernel_size, y = height, color = hit_or_miss, shape = type)) +   theme_minimal() +   labs(subtitle = \"fitted model detection vs test data with thresholding\",        caption = \"[EQ] appear as NAs\") # predict the probability output instead of class: initial_class_preds_prob_val <- predict(svm_cls_fit, valid, type = \"prob\")  # probabilities and outcome: initial_class_preds_prob_val <- initial_class_preds_prob_val |>                                   dplyr::bind_cols(valid |> dplyr::select(type))  svm_cls_fit_calibrated <- initial_class_preds_prob_val |>                           probably::cal_estimate_logistic(truth = \"type\") #> Registered S3 method overwritten by 'butcher': #>   method                 from     #>   as.character.dev_topic generics  # svm_cls_fit_calibrated calibrated_preds_on_test <- probably::cal_apply(                             initial_class_preds_prob_test,                             svm_cls_fit_calibrated,                             pred_class = \".pred_class\"                             )  # confusion matrix with calibration calibrated_preds_on_test |>    yardstick::conf_mat(type, .pred_class) #>           Truth #> Prediction Field Popcorn #>    Field      17       1 #>    Popcorn     1      21"},{"path":"https://frankiethull.github.io/maize/articles/two-class-post-processing.html","id":"detecting-class-with-maize-and-eq-thresholding-via-probably","dir":"Articles","previous_headings":"","what":"detecting class with {maize} and [EQ] thresholding via probably","title":"two-class-post-processing","text":"let’s see confusion matrix looks like given trained model. ’s probably (pun intended) edge overlapping observations. try tune parameters perfect confusion matrix. try calibrating trained model. cases handle edge, ’s uncertain predict accurately, .e. equivocal zone. classification field vs test observations:  detection equivocal zones via probably: check EQs:","code":"initial_class_preds_test <- predict(svm_cls_fit, test)  test |>   dplyr::bind_cols(initial_class_preds_test) |>   yardstick::conf_mat(type, .pred_class) #>           Truth #> Prediction Field Popcorn #>    Field      15       1 #>    Popcorn     3      21 preds <- predict(svm_cls_fit, cornfield, \"class\") predfield <- cornfield |> cbind(preds)  test |>   dplyr::bind_cols(     initial_class_preds_test   ) |>   dplyr::mutate(     hit_or_miss = type == .pred_class   ) |>   ggplot() +   # plotting the classification field:     geom_tile(inherit.aes = FALSE,             data = predfield,              aes(x = kernel_size, y = height, fill = .pred_class),             alpha = .4) +    geom_point(aes(x = kernel_size, y = height, color = hit_or_miss, shape = type)) +   theme_minimal() +   labs(subtitle = \"fitted model detection vs test data\") # predict the probability output instead of class: initial_class_preds_prob_test <- predict(svm_cls_fit, test, type = \"prob\")  # probabilities and outcome: initial_class_preds_prob_test <- initial_class_preds_prob_test |>                                   dplyr::bind_cols(test |> dplyr::select(type))  # probably EQ step: class_preds_prob_eq <- initial_class_preds_prob_test |>   dplyr::mutate(     .pred_class = probably::make_two_class_pred(       estimate = .pred_Field,       levels = levels(type),       # adjust threshold and buffer to handle eq        threshold = 0.5,       buffer = 0.35     )   )  class_preds_prob_eq |>    dplyr::count(.pred_class) #> # A tibble: 3 × 2 #>   .pred_class     n #>    <clss_prd> <int> #> 1        [EQ]     4 #> 2       Field    14 #> 3     Popcorn    22 test |>   dplyr::bind_cols(     class_preds_prob_eq |> dplyr::select(.pred_class)   ) |>   dplyr::mutate(     .pred_class = as.factor(.pred_class),     hit_or_miss = type == .pred_class   ) |>   ggplot() +   # plotting the classification field:     geom_tile(inherit.aes = FALSE,             data = predfield,              aes(x = kernel_size, y = height, fill = .pred_class),             alpha = .4) +    geom_point(aes(x = kernel_size, y = height, color = hit_or_miss, shape = type)) +   theme_minimal() +   labs(subtitle = \"fitted model detection vs test data with thresholding\",        caption = \"[EQ] appear as NAs\")"},{"path":"https://frankiethull.github.io/maize/articles/two-class-post-processing.html","id":"calibration-with-probably","dir":"Articles","previous_headings":"","what":"calibration with probably","title":"two-class-post-processing","text":"note calibration functions regression classification {probably}. ’re honing calibration classification demonstrate cal_estimate_logistic. Since calibration, run calibration step validation data, validation step used calibrate predictions test.","code":"# predict the probability output instead of class: initial_class_preds_prob_val <- predict(svm_cls_fit, valid, type = \"prob\")  # probabilities and outcome: initial_class_preds_prob_val <- initial_class_preds_prob_val |>                                   dplyr::bind_cols(valid |> dplyr::select(type))  svm_cls_fit_calibrated <- initial_class_preds_prob_val |>                           probably::cal_estimate_logistic(truth = \"type\") #> Registered S3 method overwritten by 'butcher': #>   method                 from     #>   as.character.dev_topic generics  # svm_cls_fit_calibrated calibrated_preds_on_test <- probably::cal_apply(                             initial_class_preds_prob_test,                             svm_cls_fit_calibrated,                             pred_class = \".pred_class\"                             )  # confusion matrix with calibration calibrated_preds_on_test |>    yardstick::conf_mat(type, .pred_class) #>           Truth #> Prediction Field Popcorn #>    Field      17       1 #>    Popcorn     1      21"},{"path":"https://frankiethull.github.io/maize/articles/two-class-post-processing.html","id":"tailor","dir":"Articles","previous_headings":"","what":"{tailor}","title":"two-class-post-processing","text":"experimental post-processing API layer {probably}. Making post-processing easy! {parsnip} helper engines, {recipes} pre-processing, tailor new package post-processing. Note development version workflows workflows::add_tailor() automate process. looks like post process steps registered tunable parameters !","code":"library(tailor)  post_process <- tailor() |>                 adjust_probability_calibration(method = \"logistic\") |>                 adjust_probability_threshold(threshold = 0.5) |>                 adjust_equivocal_zone(value = 0.35)  tailored_wkflow <-                 svm_wkflow |>   # add tailor adjustments to our initial workflow    workflows::add_tailor(     post_process   )  svm_cls_fit_tailor <- fit(tailored_wkflow, train, calibration = valid)  # with threshold and EQ buffer and logistic calibration svm_cls_fit_tailor |>    predict(test) |>   dplyr::bind_cols(test) |>   yardstick::conf_mat(type, .pred_class) #>           Truth #> Prediction Field Popcorn #>    Field      17       1 #>    Popcorn     0      18"},{"path":"https://frankiethull.github.io/maize/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Frank Hull. Author, maintainer. Max Kuhn. Contributor.","code":""},{"path":"https://frankiethull.github.io/maize/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Hull F (2025). maize: Specialty Kernels SVMs. R package version 0.0.2.95, https://frankiethull.github.io/maize/.","code":"@Manual{,   title = {maize: Specialty Kernels for SVMs},   author = {Frank Hull},   year = {2025},   note = {R package version 0.0.2.95},   url = {https://frankiethull.github.io/maize/}, }"},{"path":[]},{"path":"https://frankiethull.github.io/maize/readme.html","id":"maize-","dir":"","previous_headings":"","what":"maize","title":"{maize} package","text":"{maize} 🌽 extension library kernels & support vector machines tidymodels! package consists additional kernel bindings available {parsnip} {recipes} package. Many kernels ported {kernlab}, additional kernels added directly maize transposed Python Julia packages. {parnsip} three kernels available: linear, radial basis function, & polynomial. {maize} extends kernels, engines, adds steps {recipes}:","code":""},{"path":"https://frankiethull.github.io/maize/readme.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"{maize} package","text":"can install development version maize GitHub :","code":"# install.packages(\"pak\") pak::pak(\"frankiethull/maize\")"},{"path":[]},{"path":"https://frankiethull.github.io/maize/readme.html","id":"kernlab","dir":"","previous_headings":"engines","what":"{kernlab}","title":"{maize} package","text":"SVMs Specialty Kernels. Contains additional regression classification techniques LS-SVMs.","code":""},{"path":"https://frankiethull.github.io/maize/readme.html","id":"mildsvm","dir":"","previous_headings":"engines","what":"{mildsvm}","title":"{maize} package","text":"Multi-Instance Learners SVMs. particular, MIL ordinal outcomes using One-vs-.","code":""},{"path":"https://frankiethull.github.io/maize/readme.html","id":"ebmc","dir":"","previous_headings":"engines","what":"{ebmc}","title":"{maize} package","text":"Bagging Boosting weak learners via Random Sampling binary classification.","code":""},{"path":"https://frankiethull.github.io/maize/readme.html","id":"recipes","dir":"","previous_headings":"engines","what":"recipes","title":"{maize} package","text":"Steps feature engineering data via kernel related methods.","code":""},{"path":"https://frankiethull.github.io/maize/readme.html","id":"probably","dir":"","previous_headings":"engines","what":"probably","title":"{maize} package","text":"Point calibration conformal quantile regression SVMs (QRSVM) prediction intervals.","code":""},{"path":"https://frankiethull.github.io/maize/readme.html","id":"modeltime","dir":"","previous_headings":"engines","what":"modeltime","title":"{maize} package","text":"special implementation SVMs time series regression. ARIMA & AutoARIMA SVM Errors registered maize. harvestime vignette showcases --use ARIMA-SVMs & Recursive SVMs.","code":""},{"path":"https://frankiethull.github.io/maize/readme.html","id":"applicable","dir":"","previous_headings":"engines","what":"applicable","title":"{maize} package","text":"One-Class SVMs novelty detection","code":""},{"path":"https://frankiethull.github.io/maize/readme.html","id":"corrr","dir":"","previous_headings":"engines","what":"corrr","title":"{maize} package","text":"Inspired corrr, returns tidy data frame class (kcor_df) Kernel Canonical Correlation Analysis:","code":""},{"path":"https://frankiethull.github.io/maize/reference/ada_boost_svm_rbf.html","id":null,"dir":"Reference","previous_headings":"","what":"AdaBoost SVM with Radial Basis Function Kernel — ada_boost_svm_rbf","title":"AdaBoost SVM with Radial Basis Function Kernel — ada_boost_svm_rbf","text":"AdaBoost SVM Radial Basis Function Kernel","code":""},{"path":"https://frankiethull.github.io/maize/reference/ada_boost_svm_rbf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AdaBoost SVM with Radial Basis Function Kernel — ada_boost_svm_rbf","text":"","code":"ada_boost_svm_rbf(   mode = \"unknown\",   engine = \"ebmc\",   num_learners = NULL,   imb_ratio = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/ada_boost_svm_rbf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AdaBoost SVM with Radial Basis Function Kernel — ada_boost_svm_rbf","text":"mode classification engine ebmc's adam2 uses e1701's svm num_learners many weak learners ensembled via boosting imb_ratio major-minor class imbalance ratio","code":""},{"path":"https://frankiethull.github.io/maize/reference/apd_svm_novel_detection.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit an SVM novelty detection model to estimate an applicability domain. — apd_svm_novel_detection","title":"Fit an SVM novelty detection model to estimate an applicability domain. — apd_svm_novel_detection","text":"apd_svm_novel_detection() fits 'one-svc' novelty detection model.","code":""},{"path":"https://frankiethull.github.io/maize/reference/apd_svm_novel_detection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit an SVM novelty detection model to estimate an applicability domain. — apd_svm_novel_detection","text":"","code":"apd_svm_novel_detection(x, ...)  # Default S3 method apd_svm_novel_detection(x, ...)  # S3 method for class 'data.frame' apd_svm_novel_detection(x, ...)  # S3 method for class 'matrix' apd_svm_novel_detection(x, ...)  # S3 method for class 'formula' apd_svm_novel_detection(formula, data, ...)  # S3 method for class 'recipe' apd_svm_novel_detection(x, data, ...)"},{"path":"https://frankiethull.github.io/maize/reference/apd_svm_novel_detection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit an SVM novelty detection model to estimate an applicability domain. — apd_svm_novel_detection","text":"x Depending context: data frame predictors. matrix predictors. recipe specifying set preprocessing steps created recipes::recipe(). ... Options pass kernlab::ksvm(). Options include data. formula formula specifying predictor terms right-hand side. outcome specified. data recipe formula used, data specified : data frame containing predictors.","code":""},{"path":"https://frankiethull.github.io/maize/reference/apd_svm_novel_detection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit an SVM novelty detection model to estimate an applicability domain. — apd_svm_novel_detection","text":"apd_svm_novel_detection object.","code":""},{"path":"https://frankiethull.github.io/maize/reference/arima_svm_laplace.html","id":null,"dir":"Reference","previous_headings":"","what":"General Interface for ","title":"General Interface for ","text":"arima_svm_laplace() way generate specification time series model uses SVMs improve modeling errors (residuals) Exogenous Regressors. works \"automated\" ARIMA (auto.arima) standard ARIMA (arima). main algorithms : Auto ARIMA + SVM Errors (engine = auto_arima_svm_laplace, default) ARIMA + SVM Errors (engine = arima_svm_laplace)","code":""},{"path":"https://frankiethull.github.io/maize/reference/arima_svm_laplace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General Interface for ","text":"","code":"arima_svm_laplace(   mode = \"regression\",   seasonal_period = NULL,   non_seasonal_ar = NULL,   non_seasonal_differences = NULL,   non_seasonal_ma = NULL,   seasonal_ar = NULL,   seasonal_differences = NULL,   seasonal_ma = NULL,   cost = NULL,   margin = NULL,   laplace_sigma = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/arima_svm_laplace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General Interface for ","text":"mode single character string type model. possible value model \"regression\". seasonal_period seasonal frequency. Uses \"auto\" default. character phrase \"auto\" time-based phrase \"2 weeks\" can used date date-time variable provided. See Fit Details . non_seasonal_ar order non-seasonal auto-regressive (AR) terms. Often denoted \"p\" pdq-notation. non_seasonal_differences order integration non-seasonal differencing. Often denoted \"d\" pdq-notation. non_seasonal_ma order non-seasonal moving average (MA) terms. Often denoted \"q\" pdq-notation. seasonal_ar order seasonal auto-regressive (SAR) terms. Often denoted \"P\" PDQ-notation. seasonal_differences order integration seasonal differencing. Often denoted \"D\" PDQ-notation. seasonal_ma order seasonal moving average (SMA) terms. Often denoted \"Q\" PDQ-notation. cost positive number cost predicting sample within wrong side margin margin positive number epsilon SVM insensitive loss function (regression ) laplace_sigma sigma parameter laplacian sample_size number number (proportion) data exposed fitting routine.","code":""},{"path":"https://frankiethull.github.io/maize/reference/autoplot.kcor_df.html","id":null,"dir":"Reference","previous_headings":"","what":"visualize component x-y feature space — autoplot.kcor_df","title":"visualize component x-y feature space — autoplot.kcor_df","text":"visualize component x-y feature space","code":""},{"path":"https://frankiethull.github.io/maize/reference/autoplot.kcor_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"visualize component x-y feature space — autoplot.kcor_df","text":"","code":"# S3 method for class 'kcor_df' autoplot(x)"},{"path":"https://frankiethull.github.io/maize/reference/autoplot.kcor_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"visualize component x-y feature space — autoplot.kcor_df","text":"x kcor_df object","code":""},{"path":"https://frankiethull.github.io/maize/reference/bag_svm_rbf.html","id":null,"dir":"Reference","previous_headings":"","what":"RUS Bagged SVM with Radial Basis Function Kernel — bag_svm_rbf","title":"RUS Bagged SVM with Radial Basis Function Kernel — bag_svm_rbf","text":"RUS Bagged SVM Radial Basis Function Kernel","code":""},{"path":"https://frankiethull.github.io/maize/reference/bag_svm_rbf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"RUS Bagged SVM with Radial Basis Function Kernel — bag_svm_rbf","text":"","code":"bag_svm_rbf(   mode = \"unknown\",   engine = \"ebmc\",   num_learners = NULL,   imb_ratio = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/bag_svm_rbf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"RUS Bagged SVM with Radial Basis Function Kernel — bag_svm_rbf","text":"mode classification engine ebmc's ub uses e1701's svm num_learners many weak learners ensembled via bagging imb_ratio major-minor class imbalance ratio","code":""},{"path":"https://frankiethull.github.io/maize/reference/cal_estimate_svm.html","id":null,"dir":"Reference","previous_headings":"","what":"Uses a support vector regression model to calibrate numeric predictions — cal_estimate_svm","title":"Uses a support vector regression model to calibrate numeric predictions — cal_estimate_svm","text":"Uses support vector regression model calibrate numeric predictions","code":""},{"path":"https://frankiethull.github.io/maize/reference/cal_estimate_svm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Uses a support vector regression model to calibrate numeric predictions — cal_estimate_svm","text":"","code":"cal_estimate_svm(   .data,   truth = NULL,   estimate = dplyr::matches(\"^.pred$\"),   smooth = TRUE,   parameters = NULL,   ...,   .by = NULL )  # S3 method for class 'data.frame' cal_estimate_svm(   .data,   truth = NULL,   estimate = dplyr::matches(\"^.pred$\"),   smooth = TRUE,   parameters = NULL,   ...,   .by = NULL )  # S3 method for class 'tune_results' cal_estimate_svm(   .data,   truth = NULL,   estimate = dplyr::matches(\"^.pred$\"),   smooth = TRUE,   parameters = NULL,   ... )  # S3 method for class 'grouped_df' cal_estimate_svm(   .data,   truth = NULL,   estimate = NULL,   smooth = TRUE,   parameters = NULL,   ... )"},{"path":"https://frankiethull.github.io/maize/reference/cal_estimate_svm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Uses a support vector regression model to calibrate numeric predictions — cal_estimate_svm","text":".data ungrouped  data.frame object, tune_results object, contains prediction column. truth column identifier observed outcome data (numeric). unquoted column name. estimate Column identifier predicted values smooth Applies svm models. switches polydotTRUE, vanilladot FALSE. parameters (Optional)  optional tibble tuning parameter values can used filter predicted values processing. Applies tune_results objects. ... Additional arguments passed models routines used calculate new predictions.","code":""},{"path":"https://frankiethull.github.io/maize/reference/cal_estimate_svm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Uses a support vector regression model to calibrate numeric predictions — cal_estimate_svm","text":"function uses existing modeling functions packages create calibration: kernlab::ksvm() \"vanilladot\" used smooth set FALSE kernlab::ksvm() \"polydot\" used smooth set TRUE methods estimate relationship unmodified predicted values remove trend cal_apply() invoked.","code":""},{"path":[]},{"path":"https://frankiethull.github.io/maize/reference/corn_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Synthetic Corn Dataset for Corny Example — corn_data","title":"Synthetic Corn Dataset for Corny Example — corn_data","text":"Synthetic Corn Dataset Corny Example","code":""},{"path":"https://frankiethull.github.io/maize/reference/corn_data.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Synthetic Corn Dataset for Corny Example — corn_data","text":"claude-3-5-sonnet-20240620","code":""},{"path":"https://frankiethull.github.io/maize/reference/corn_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Synthetic Corn Dataset for Corny Example — corn_data","text":"corn_data tibble","code":""},{"path":"https://frankiethull.github.io/maize/reference/corn_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Synthetic Corn Dataset for Corny Example — corn_data","text":"Asked Claude Sonnet corn data given README story problem","code":""},{"path":"https://frankiethull.github.io/maize/reference/corn_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Synthetic Corn Dataset for Corny Example — corn_data","text":"","code":"data(corn_data) str(corn_data) #> tibble [300 × 3] (S3: tbl_df/tbl/data.frame) #>  $ height     : num [1:300] 59.5 57.3 55.4 52.6 61 ... #>  $ kernel_size: num [1:300] 6.53 9.7 8.48 10.29 7.53 ... #>  $ type       : Factor w/ 3 levels \"Field\",\"Popcorn\",..: 3 3 3 3 3 3 3 3 3 3 ..."},{"path":"https://frankiethull.github.io/maize/reference/int_conformal_quantile_svm.html","id":null,"dir":"Reference","previous_headings":"","what":"Prediction intervals via conformal inference and QRSVM — int_conformal_quantile_svm","title":"Prediction intervals via conformal inference and QRSVM — int_conformal_quantile_svm","text":"compute quantiles, function uses Quantile SVM instead probably's \"int_conformal_quantile\" QRF classic quantile regression.","code":""},{"path":"https://frankiethull.github.io/maize/reference/int_conformal_quantile_svm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prediction intervals via conformal inference and QRSVM — int_conformal_quantile_svm","text":"","code":"int_conformal_quantile_svm(object, ...)  # S3 method for class 'workflow' int_conformal_quantile_svm(object, train_data, cal_data, level = 0.95, ...)"},{"path":"https://frankiethull.github.io/maize/reference/int_conformal_quantile_svm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prediction intervals via conformal inference and QRSVM — int_conformal_quantile_svm","text":"object fitted workflows::workflow() object. ... Options pass qrsvm::qrsvm() train_data, cal_data Data frames predictor outcome data. train_data data used produce object cal_data used produce predictions (residuals). workflow used recipe, data inputs recipe (product recipe). level confidence level intervals.","code":""},{"path":"https://frankiethull.github.io/maize/reference/int_conformal_quantile_svm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prediction intervals via conformal inference and QRSVM — int_conformal_quantile_svm","text":"object class \"int_conformal_quantile\" containing information create intervals (includes object). predict() method used produce intervals.","code":""},{"path":"https://frankiethull.github.io/maize/reference/int_conformal_quantile_svm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prediction intervals via conformal inference and QRSVM — int_conformal_quantile_svm","text":"based initial probably implementation slight modification. origin: https://github.com/tidymodels/probably/blob/HEAD/R/conformal_infer_quantile.R information, visit: https://probably.tidymodels.org","code":""},{"path":"https://frankiethull.github.io/maize/reference/kcca_correlate.html","id":null,"dir":"Reference","previous_headings":"","what":"Kernel Canonical Correlation Analysis — kcca_correlate","title":"Kernel Canonical Correlation Analysis — kcca_correlate","text":"Computes canonical correlation analysis feature space. Kernel Canonical Correlation Analysis (KCCA) non-linear extension CCA. Given two random variables (datasets), KCCA aims extracting information shared two random variables (datasets). information found kernlab::kcca()","code":""},{"path":"https://frankiethull.github.io/maize/reference/kcca_correlate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kernel Canonical Correlation Analysis — kcca_correlate","text":"","code":"kcca_correlate(x, y = NULL, kernel = \"rbfdot\", gamma = 0.1, num_comp = 10, ...)"},{"path":"https://frankiethull.github.io/maize/reference/kcca_correlate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kernel Canonical Correlation Analysis — kcca_correlate","text":"x variable dataframe y variable dataframe kernel kernel use gamma regularization parameter num_comp number components ... pass args kcca function","code":""},{"path":"https://frankiethull.github.io/maize/reference/kcca_correlate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kernel Canonical Correlation Analysis — kcca_correlate","text":"kernel canonical correlation analysis data frame kcor_df","code":""},{"path":"https://frankiethull.github.io/maize/reference/kqr_laplace.html","id":null,"dir":"Reference","previous_headings":"","what":"Laplacian Kernel Quantile Regression — kqr_laplace","title":"Laplacian Kernel Quantile Regression — kqr_laplace","text":"laplacian kernel","code":""},{"path":"https://frankiethull.github.io/maize/reference/kqr_laplace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Laplacian Kernel Quantile Regression — kqr_laplace","text":"","code":"kqr_laplace(   mode = \"unknown\",   engine = \"kernlab\",   cost = NULL,   tau = NULL,   laplace_sigma = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/kqr_laplace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Laplacian Kernel Quantile Regression — kqr_laplace","text":"mode regression engine kernlab kqr cost positive number cost predicting sample within wrong side margin tau quantile loss function laplace_sigma sigma parameter laplacian","code":""},{"path":"https://frankiethull.github.io/maize/reference/lssvm_laplace.html","id":null,"dir":"Reference","previous_headings":"","what":"Laplacian Least Squares Support Vector Machine — lssvm_laplace","title":"Laplacian Least Squares Support Vector Machine — lssvm_laplace","text":"laplacian kernel","code":""},{"path":"https://frankiethull.github.io/maize/reference/lssvm_laplace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Laplacian Least Squares Support Vector Machine — lssvm_laplace","text":"","code":"lssvm_laplace(mode = \"unknown\", engine = \"kernlab\", laplace_sigma = NULL)"},{"path":"https://frankiethull.github.io/maize/reference/lssvm_laplace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Laplacian Least Squares Support Vector Machine — lssvm_laplace","text":"mode classification engine kernlab lssvm laplace_sigma sigma parameter laplacian","code":""},{"path":"https://frankiethull.github.io/maize/reference/maize-package.html","id":null,"dir":"Reference","previous_headings":"","what":"maize: Specialty Kernels for SVMs — maize-package","title":"maize: Specialty Kernels for SVMs — maize-package","text":"Bindings SVMs special kernels via kernlab, e1701, ebmc, mildsvm extending 'parsnip' package. Specifically related kernels support vector machines available parsnip. Package also includes interfaces novel SVM preprocessors (recipes) postprocessors (probably).","code":""},{"path":[]},{"path":"https://frankiethull.github.io/maize/reference/maize-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"maize: Specialty Kernels for SVMs — maize-package","text":"Maintainer: Frank Hull frankiethull@gmail.com contributors: Max Kuhn max@posit.co [contributor]","code":""},{"path":"https://frankiethull.github.io/maize/reference/misvm_orova_rbf.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiple Instance SVMs for Ordinal Outcome Data, One-Vs-All, with Radial Basis Function Kernel — misvm_orova_rbf","title":"Multiple Instance SVMs for Ordinal Outcome Data, One-Vs-All, with Radial Basis Function Kernel — misvm_orova_rbf","text":"Multiple Instance SVMs Ordinal Outcome Data, One-Vs-, Radial Basis Function Kernel","code":""},{"path":"https://frankiethull.github.io/maize/reference/misvm_orova_rbf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiple Instance SVMs for Ordinal Outcome Data, One-Vs-All, with Radial Basis Function Kernel — misvm_orova_rbf","text":"","code":"misvm_orova_rbf(   mode = \"unknown\",   engine = \"ebmc\",   cost = NULL,   rbf_sigma = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/misvm_orova_rbf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiple Instance SVMs for Ordinal Outcome Data, One-Vs-All, with Radial Basis Function Kernel — misvm_orova_rbf","text":"mode classification engine mildsvm::misvm_orova() uses e1701's svm cost positive number cost predicting sample within wrong side margin rbf_sigma positive number radial basis function.","code":""},{"path":"https://frankiethull.github.io/maize/reference/rus_boost_svm_rbf.html","id":null,"dir":"Reference","previous_headings":"","what":"RUSBoost SVM with Radial Basis Function Kernel — rus_boost_svm_rbf","title":"RUSBoost SVM with Radial Basis Function Kernel — rus_boost_svm_rbf","text":"RUSBoost SVM Radial Basis Function Kernel","code":""},{"path":"https://frankiethull.github.io/maize/reference/rus_boost_svm_rbf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"RUSBoost SVM with Radial Basis Function Kernel — rus_boost_svm_rbf","text":"","code":"rus_boost_svm_rbf(   mode = \"unknown\",   engine = \"ebmc\",   num_learners = NULL,   imb_ratio = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/rus_boost_svm_rbf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"RUSBoost SVM with Radial Basis Function Kernel — rus_boost_svm_rbf","text":"mode classification engine ebmc's rus uses e1701's svm num_learners many weak learners ensembled via boosting imb_ratio major-minor class imbalance ratio","code":""},{"path":"https://frankiethull.github.io/maize/reference/rvm_laplace.html","id":null,"dir":"Reference","previous_headings":"","what":"Laplacian Relevance Vector Machine (Experimental RVM) — rvm_laplace","title":"Laplacian Relevance Vector Machine (Experimental RVM) — rvm_laplace","text":"laplacian kernel","code":""},{"path":"https://frankiethull.github.io/maize/reference/rvm_laplace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Laplacian Relevance Vector Machine (Experimental RVM) — rvm_laplace","text":"","code":"rvm_laplace(   mode = \"unknown\",   engine = \"kernlab\",   alpha = NULL,   var = NULL,   laplace_sigma = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/rvm_laplace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Laplacian Relevance Vector Machine (Experimental RVM) — rvm_laplace","text":"mode regression RVM engine kernlab rvm alpha (alpha) initial alpha value vector. Can either vector length equal number data points single number. var (var) initial noise variance laplace_sigma sigma parameter laplacian","code":""},{"path":"https://frankiethull.github.io/maize/reference/score.apd_svm_novel_detection.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict from a apd_svm_novel_detection — score.apd_svm_novel_detection","title":"Predict from a apd_svm_novel_detection — score.apd_svm_novel_detection","text":"Predict apd_svm_novel_detection","code":""},{"path":"https://frankiethull.github.io/maize/reference/score.apd_svm_novel_detection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict from a apd_svm_novel_detection — score.apd_svm_novel_detection","text":"","code":"# S3 method for class 'apd_svm_novel_detection' score(object, new_data, type = \"numeric\", ...)"},{"path":"https://frankiethull.github.io/maize/reference/score.apd_svm_novel_detection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict from a apd_svm_novel_detection — score.apd_svm_novel_detection","text":"object apd_svm_novel_detection object. new_data data frame matrix new samples. type single character. type predictions generate. Valid options : \"numeric\" numeric predictions. ... used, required extensibility.","code":""},{"path":"https://frankiethull.github.io/maize/reference/score.apd_svm_novel_detection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict from a apd_svm_novel_detection — score.apd_svm_novel_detection","text":"tibble predictions. number rows tibble guaranteed number rows new_data. score column raw prediction kernlab::predict() score_pctl compares value reference distribution score created predicting training set. value X means X percent training data scores less predicted value.","code":""},{"path":"https://frankiethull.github.io/maize/reference/score.apd_svm_novel_detection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict from a apd_svm_novel_detection — score.apd_svm_novel_detection","text":"score","code":""},{"path":[]},{"path":"https://frankiethull.github.io/maize/reference/step_kfa_laplace.html","id":null,"dir":"Reference","previous_headings":"","what":"Laplacian function kernel KFA signal extraction — step_kfa_laplace","title":"Laplacian function kernel KFA signal extraction — step_kfa_laplace","text":"step_kfa_laplace() creates specification recipe step convert numeric data one kernel components using laplace kernel. similar KPCA, instead extracting eigenvectors dataset feature space, approximates eigenvectors selecting patterns good basis vectors dataset.","code":""},{"path":"https://frankiethull.github.io/maize/reference/step_kfa_laplace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Laplacian function kernel KFA signal extraction — step_kfa_laplace","text":"","code":"step_kfa_laplace(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   num_comp = 5,   res = NULL,   columns = NULL,   sigma = 0.2,   prefix = \"kFA\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"kfa_laplace\") )"},{"path":"https://frankiethull.github.io/maize/reference/step_kfa_laplace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Laplacian function kernel KFA signal extraction — step_kfa_laplace","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables step. See selections() details. role model terms created step, analysis role assigned? default, new columns created step original variables used predictors model. trained logical indicate quantities preprocessing estimated. num_comp number components retain new predictors. num_comp greater number columns number possible components, smaller value used. num_comp = 0 set transformation done selected variables stay unchanged, regardless value keep_original_cols. res S4 kernlab::kfa() object stored preprocessing step trained prep(). columns character string selected variable names. field placeholder populated prep() used. sigma numeric value laplace function parameter. prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked bake()? operations baked prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify .","code":""},{"path":[]},{"path":"https://frankiethull.github.io/maize/reference/step_kfm_nystrom.html","id":null,"dir":"Reference","previous_headings":"","what":"Nystrom kernel feature map approximation (RBF) — step_kfm_nystrom","title":"Nystrom kernel feature map approximation (RBF) — step_kfm_nystrom","text":"step_kfm_nystrom() creates specification recipe step convert numeric data feature appoximation. nystrom approximates 'radial' kernel approximation.","code":""},{"path":"https://frankiethull.github.io/maize/reference/step_kfm_nystrom.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nystrom kernel feature map approximation (RBF) — step_kfm_nystrom","text":"","code":"step_kfm_nystrom(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   res = NULL,   columns = NULL,   sigma = 0.2,   m = NULL,   r = NULL,   sampling = \"random\",   prefix = \"kFM\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"kfm_nystrom\") )"},{"path":"https://frankiethull.github.io/maize/reference/step_kfm_nystrom.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Nystrom kernel feature map approximation (RBF) — step_kfm_nystrom","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables step. See selections() details. role model terms created step, analysis role assigned? default, new columns created step original variables used predictors model. trained logical indicate quantities preprocessing estimated. res mildsvm::kfm_nystrom() object stored preprocessing step trained prep(). columns character string selected variable names. field placeholder populated prep() used. sigma numeric value nystrom function parameter. m number rows df sample fitting. defaults nrow data r rank matrix approximation use. Must less equal m, default. prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked bake()? operations baked prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify .","code":""},{"path":[]},{"path":"https://frankiethull.github.io/maize/reference/step_kha_laplace.html","id":null,"dir":"Reference","previous_headings":"","what":"Laplacian function kernel PCA signal extraction via Hebbian Algorithm — step_kha_laplace","title":"Laplacian function kernel PCA signal extraction via Hebbian Algorithm — step_kha_laplace","text":"step_kha_laplace() creates specification recipe step convert numeric data one principal components using laplace kernel basis expansion.","code":""},{"path":"https://frankiethull.github.io/maize/reference/step_kha_laplace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Laplacian function kernel PCA signal extraction via Hebbian Algorithm — step_kha_laplace","text":"","code":"step_kha_laplace(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   num_comp = 5,   res = NULL,   columns = NULL,   sigma = 0.2,   learn_rate = 0.005,   threshold = 1e-04,   stop_iter = 100,   prefix = \"kha\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"kha_laplace\") )"},{"path":"https://frankiethull.github.io/maize/reference/step_kha_laplace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Laplacian function kernel PCA signal extraction via Hebbian Algorithm — step_kha_laplace","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables step. See selections() details. role model terms created step, analysis role assigned? default, new columns created step original variables used predictors model. trained logical indicate quantities preprocessing estimated. num_comp number components retain new predictors. num_comp greater number columns number possible components, smaller value used. num_comp = 0 set transformation done selected variables stay unchanged, regardless value keep_original_cols. res S4 kernlab::kha() object stored preprocessing step trained prep(). columns character string selected variable names. field placeholder populated prep() used. sigma numeric value laplace function parameter. learn_rate hebbian learning rate threshold smallest value convergence step stop_iter maximum number iterations prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked bake()? operations baked prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify .","code":""},{"path":[]},{"path":"https://frankiethull.github.io/maize/reference/step_kha_tanh.html","id":null,"dir":"Reference","previous_headings":"","what":"tanh function kernel PCA signal extraction via Hebbian Algorithm — step_kha_tanh","title":"tanh function kernel PCA signal extraction via Hebbian Algorithm — step_kha_tanh","text":"step_kha_tanh() creates specification recipe step convert numeric data one principal components using tanh kernel basis expansion.","code":""},{"path":"https://frankiethull.github.io/maize/reference/step_kha_tanh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"tanh function kernel PCA signal extraction via Hebbian Algorithm — step_kha_tanh","text":"","code":"step_kha_tanh(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   num_comp = 5,   res = NULL,   columns = NULL,   scale_factor = 0.2,   learn_rate = 0.005,   threshold = 1e-06,   stop_iter = 100,   prefix = \"kha\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"kha_tanh\") )"},{"path":"https://frankiethull.github.io/maize/reference/step_kha_tanh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"tanh function kernel PCA signal extraction via Hebbian Algorithm — step_kha_tanh","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables step. See selections() details. role model terms created step, analysis role assigned? default, new columns created step original variables used predictors model. trained logical indicate quantities preprocessing estimated. num_comp number components retain new predictors. num_comp greater number columns number possible components, smaller value used. num_comp = 0 set transformation done selected variables stay unchanged, regardless value keep_original_cols. res S4 kernlab::kha() object stored preprocessing step trained prep(). columns character string selected variable names. field placeholder populated prep() used. scale_factor numeric value tanh function parameter. learn_rate hebbian learning rate threshold smallest value convergence step stop_iter maximum number iterations prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked bake()? operations baked prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify .","code":""},{"path":[]},{"path":"https://frankiethull.github.io/maize/reference/step_kpca_laplace.html","id":null,"dir":"Reference","previous_headings":"","what":"Laplacian function kernel PCA signal extraction — step_kpca_laplace","title":"Laplacian function kernel PCA signal extraction — step_kpca_laplace","text":"step_kpca_laplace() creates specification recipe step convert numeric data one principal components using laplace kernel","code":""},{"path":"https://frankiethull.github.io/maize/reference/step_kpca_laplace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Laplacian function kernel PCA signal extraction — step_kpca_laplace","text":"","code":"step_kpca_laplace(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   num_comp = 5,   res = NULL,   columns = NULL,   sigma = 0.2,   prefix = \"kPC\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"kpca_laplace\") )"},{"path":"https://frankiethull.github.io/maize/reference/step_kpca_laplace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Laplacian function kernel PCA signal extraction — step_kpca_laplace","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables step. See selections() details. role model terms created step, analysis role assigned? default, new columns created step original variables used predictors model. trained logical indicate quantities preprocessing estimated. num_comp number components retain new predictors. num_comp greater number columns number possible components, smaller value used. num_comp = 0 set transformation done selected variables stay unchanged, regardless value keep_original_cols. res S4 kernlab::kpca() object stored preprocessing step trained prep(). columns character string selected variable names. field placeholder populated prep() used. sigma numeric value laplace function parameter. prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked bake()? operations baked prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify .","code":""},{"path":[]},{"path":"https://frankiethull.github.io/maize/reference/step_kpca_tanh.html","id":null,"dir":"Reference","previous_headings":"","what":"Laplacian function kernel PCA signal extraction — step_kpca_tanh","title":"Laplacian function kernel PCA signal extraction — step_kpca_tanh","text":"step_kpca_tanh() creates specification recipe step convert numeric data one principal components using tanh kernel basis expansion.","code":""},{"path":"https://frankiethull.github.io/maize/reference/step_kpca_tanh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Laplacian function kernel PCA signal extraction — step_kpca_tanh","text":"","code":"step_kpca_tanh(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   num_comp = 5,   res = NULL,   columns = NULL,   scale_factor = 0.2,   offset = 0,   prefix = \"kPC\",   keep_original_cols = FALSE,   skip = FALSE,   id = rand_id(\"kpca_tanh\") )"},{"path":"https://frankiethull.github.io/maize/reference/step_kpca_tanh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Laplacian function kernel PCA signal extraction — step_kpca_tanh","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables step. See selections() details. role model terms created step, analysis role assigned? default, new columns created step original variables used predictors model. trained logical indicate quantities preprocessing estimated. num_comp number components retain new predictors. num_comp greater number columns number possible components, smaller value used. num_comp = 0 set transformation done selected variables stay unchanged, regardless value keep_original_cols. res S4 kernlab::kpca() object stored preprocessing step trained prep(). columns character string selected variable names. field placeholder populated prep() used. scale_factor numeric value tanh function parameter. prefix character string prefix resulting new variables. See notes . keep_original_cols logical keep original variables output. Defaults FALSE. skip logical. step skipped recipe baked bake()? operations baked prep() run, operations may able conducted new data (e.g. processing outcome variable(s)). Care taken using skip = TRUE may affect computations subsequent operations. id character string unique step identify .","code":""},{"path":[]},{"path":"https://frankiethull.github.io/maize/reference/svm_anova_rbf.html","id":null,"dir":"Reference","previous_headings":"","what":"ANOVA RBF Support Vector Machine — svm_anova_rbf","title":"ANOVA RBF Support Vector Machine — svm_anova_rbf","text":"anova rbf support vector machines","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_anova_rbf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ANOVA RBF Support Vector Machine — svm_anova_rbf","text":"","code":"svm_anova_rbf(   mode = \"unknown\",   engine = \"kernlab\",   cost = NULL,   anova_rbf_sigma = NULL,   degree = NULL,   margin = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/svm_anova_rbf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ANOVA RBF Support Vector Machine — svm_anova_rbf","text":"mode regression classification engine kernlab ksvm cost positive number cost predicting sample within wrong side margin anova_rbf_sigma sigma parameter anova rbf degree degree parameter anova rbf margin positive number epsilon SVM insensitive loss function (regression )","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_bessel.html","id":null,"dir":"Reference","previous_headings":"","what":"Bessel Support Vector Machine — svm_bessel","title":"Bessel Support Vector Machine — svm_bessel","text":"bessel kernel support vector machines","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_bessel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bessel Support Vector Machine — svm_bessel","text":"","code":"svm_bessel(   mode = \"unknown\",   engine = \"kernlab\",   cost = NULL,   bessel_sigma = NULL,   degree = NULL,   order = NULL,   margin = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/svm_bessel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bessel Support Vector Machine — svm_bessel","text":"mode regression classification engine kernlab ksvm cost positive number cost predicting sample within wrong side margin bessel_sigma sigma parameter bessel degree degree parameter bessel order order parameter bessel margin positive number epsilon SVM insensitive loss function (regression )","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_cauchy.html","id":null,"dir":"Reference","previous_headings":"","what":"Cauchy Support Vector Machine — svm_cauchy","title":"Cauchy Support Vector Machine — svm_cauchy","text":"cauchy kernel support vector machines","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_cauchy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cauchy Support Vector Machine — svm_cauchy","text":"","code":"svm_cauchy(   mode = \"unknown\",   engine = \"kernlab\",   cost = NULL,   margin = NULL,   sigma = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/svm_cauchy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cauchy Support Vector Machine — svm_cauchy","text":"mode regression classification engine kernlab ksvm cost positive number cost predicting sample within wrong side margin margin positive number epsilon SVM insensitive loss function (regression ) sigma sigma parameter cauchy kernels","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_cossim.html","id":null,"dir":"Reference","previous_headings":"","what":"Cosine Similarity Support Vector Machine — svm_cossim","title":"Cosine Similarity Support Vector Machine — svm_cossim","text":"cossim kernel support vector machines","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_cossim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cosine Similarity Support Vector Machine — svm_cossim","text":"","code":"svm_cossim(mode = \"unknown\", engine = \"kernlab\", cost = NULL, margin = NULL)"},{"path":"https://frankiethull.github.io/maize/reference/svm_cossim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cosine Similarity Support Vector Machine — svm_cossim","text":"mode regression classification engine kernlab ksvm cost positive number cost predicting sample within wrong side margin margin positive number epsilon SVM insensitive loss function (regression )","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_fourier.html","id":null,"dir":"Reference","previous_headings":"","what":"Fourier Support Vector Machine — svm_fourier","title":"Fourier Support Vector Machine — svm_fourier","text":"fourier kernel support vector machines","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_fourier.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fourier Support Vector Machine — svm_fourier","text":"","code":"svm_fourier(   mode = \"unknown\",   engine = \"kernlab\",   cost = NULL,   margin = NULL,   sigma = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/svm_fourier.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fourier Support Vector Machine — svm_fourier","text":"mode regression classification engine kernlab ksvm cost positive number cost predicting sample within wrong side margin margin positive number epsilon SVM insensitive loss function (regression ) sigma sigma parameter fourier kernels","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_laplace.html","id":null,"dir":"Reference","previous_headings":"","what":"Laplacian Support Vector Machine — svm_laplace","title":"Laplacian Support Vector Machine — svm_laplace","text":"laplacian kernel support vector machines","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_laplace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Laplacian Support Vector Machine — svm_laplace","text":"","code":"svm_laplace(   mode = \"unknown\",   engine = \"kernlab\",   cost = NULL,   margin = NULL,   laplace_sigma = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/svm_laplace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Laplacian Support Vector Machine — svm_laplace","text":"mode regression classification engine kernlab ksvm cost positive number cost predicting sample within wrong side margin margin positive number epsilon SVM insensitive loss function (regression ) laplace_sigma sigma parameter laplacian","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_sorensen.html","id":null,"dir":"Reference","previous_headings":"","what":"Sorensen Support Vector Machine — svm_sorensen","title":"Sorensen Support Vector Machine — svm_sorensen","text":"sorensen kernel support vector machines used graph kernel chemical informatics","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_sorensen.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sorensen Support Vector Machine — svm_sorensen","text":"","code":"svm_sorensen(mode = \"unknown\", engine = \"kernlab\", cost = NULL, margin = NULL)"},{"path":"https://frankiethull.github.io/maize/reference/svm_sorensen.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sorensen Support Vector Machine — svm_sorensen","text":"mode regression classification engine kernlab ksvm cost positive number cost predicting sample within wrong side margin margin positive number epsilon SVM insensitive loss function (regression )","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_spline.html","id":null,"dir":"Reference","previous_headings":"","what":"Spline Support Vector Machine — svm_spline","title":"Spline Support Vector Machine — svm_spline","text":"spline kernel support vector machines","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_spline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spline Support Vector Machine — svm_spline","text":"","code":"svm_spline(mode = \"unknown\", engine = \"kernlab\", cost = NULL, margin = NULL)"},{"path":"https://frankiethull.github.io/maize/reference/svm_spline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Spline Support Vector Machine — svm_spline","text":"mode regression classification engine kernlab ksvm cost positive number cost predicting sample within wrong side margin margin positive number epsilon SVM insensitive loss function (regression )","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_string.html","id":null,"dir":"Reference","previous_headings":"","what":"String Support Vector Machine — svm_string","title":"String Support Vector Machine — svm_string","text":"stringdot support vector machines","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_string.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"String Support Vector Machine — svm_string","text":"","code":"svm_string(   x,   y,   mode = \"unknown\",   engine = \"kernlab\",   cost = NULL,   length = NULL,   lambda = NULL,   normalized = TRUE,   margin = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/svm_string.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"String Support Vector Machine — svm_string","text":"mode regression classification engine kernlab ksvm cost positive number cost predicting sample within wrong side margin length length substrings considered lambda decay factor normalized normalize string kernel values, (default = TRUE) margin positive number epsilon SVM insensitive loss function (regression )","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_tanh.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperbolic Tangent Support Vector Machine — svm_tanh","title":"Hyperbolic Tangent Support Vector Machine — svm_tanh","text":"tanh kernel support vector machines","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_tanh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperbolic Tangent Support Vector Machine — svm_tanh","text":"","code":"svm_tanh(   mode = \"unknown\",   engine = \"kernlab\",   cost = NULL,   scale_factor = NULL,   margin = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/svm_tanh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperbolic Tangent Support Vector Machine — svm_tanh","text":"mode regression classification engine kernlab ksvm cost positive number cost predicting sample within wrong side margin scale_factor scale parameter tanh margin positive number epsilon SVM insensitive loss function (regression )","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_tanimoto.html","id":null,"dir":"Reference","previous_headings":"","what":"Tanimoto Support Vector Machine — svm_tanimoto","title":"Tanimoto Support Vector Machine — svm_tanimoto","text":"tanimoto kernel support vector machines used graph kernel chemical informatics","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_tanimoto.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tanimoto Support Vector Machine — svm_tanimoto","text":"","code":"svm_tanimoto(mode = \"unknown\", engine = \"kernlab\", cost = NULL, margin = NULL)"},{"path":"https://frankiethull.github.io/maize/reference/svm_tanimoto.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tanimoto Support Vector Machine — svm_tanimoto","text":"mode regression classification engine kernlab ksvm cost positive number cost predicting sample within wrong side margin margin positive number epsilon SVM insensitive loss function (regression )","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_tstudent.html","id":null,"dir":"Reference","previous_headings":"","what":"T-Student Support Vector Machine — svm_tstudent","title":"T-Student Support Vector Machine — svm_tstudent","text":"t-student kernel support vector machines","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_tstudent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"T-Student Support Vector Machine — svm_tstudent","text":"","code":"svm_tstudent(   mode = \"unknown\",   engine = \"kernlab\",   cost = NULL,   margin = NULL,   degree = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/svm_tstudent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"T-Student Support Vector Machine — svm_tstudent","text":"mode regression classification engine kernlab ksvm cost positive number cost predicting sample within wrong side margin margin positive number epsilon SVM insensitive loss function (regression ) degree degree parameter tstudent kernels","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_wavelet.html","id":null,"dir":"Reference","previous_headings":"","what":"Wavelet Support Vector Machine — svm_wavelet","title":"Wavelet Support Vector Machine — svm_wavelet","text":"wavelet kernel support vector machines","code":""},{"path":"https://frankiethull.github.io/maize/reference/svm_wavelet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wavelet Support Vector Machine — svm_wavelet","text":"","code":"svm_wavelet(   mode = \"unknown\",   engine = \"kernlab\",   cost = NULL,   margin = NULL,   sigma = NULL,   a = 1,   c = NULL,   h = NULL )"},{"path":"https://frankiethull.github.io/maize/reference/svm_wavelet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wavelet Support Vector Machine — svm_wavelet","text":"mode regression classification engine kernlab ksvm cost positive number cost predicting sample within wrong side margin margin positive number epsilon SVM insensitive loss function (regression ) sigma sigma parameter svm wavelet kernel scale adjustment parameter wavelet kernels (temp name) c dist adjustment parameter wavelet kernels can NULL (temp name) h wavelet function wavelet kernel, default wavelet NULL (temp name)","code":""}]
